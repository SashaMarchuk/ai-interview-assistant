---
phase: 16-reasoning-models
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - src/services/llm/providers/LLMProvider.ts
  - src/services/llm/providers/OpenAIProvider.ts
  - src/services/llm/providers/OpenRouterProvider.ts
  - src/services/llm/providers/streamSSE.ts
  - src/services/llm/types.ts
  - src/services/llm/index.ts
  - src/store/types.ts
  - src/store/settingsSlice.ts
  - src/store/index.ts
autonomous: true

must_haves:
  truths:
    - "Reasoning models (o3-mini, o4-mini, o1, o3) and GPT-5 series (gpt-5, gpt-5-mini, gpt-5-nano) appear in the provider model lists"
    - "Deprecated models (o1-preview, gpt-3.5-turbo, gpt-4-turbo, gpt-4) are removed from model lists"
    - "Reasoning model requests use developer role instead of system role"
    - "Reasoning model requests use max_completion_tokens with minimum 25K budget"
    - "Reasoning model requests include reasoning_effort when specified"
    - "Reasoning model requests exclude temperature and top_p parameters"
    - "Store has reasoningEffort setting with low/medium/high values, persisted"
  artifacts:
    - path: "src/services/llm/providers/LLMProvider.ts"
      provides: "ReasoningEffort type, extended ProviderStreamOptions with reasoningEffort field, isReasoningModel utility"
      contains: "ReasoningEffort"
    - path: "src/services/llm/providers/OpenAIProvider.ts"
      provides: "Updated model list with o4-mini, GPT-5 series; reasoning-aware request body"
      contains: "o4-mini"
    - path: "src/services/llm/providers/OpenRouterProvider.ts"
      provides: "Updated model list with reasoning models; reasoning-aware request body"
      contains: "developer"
    - path: "src/store/types.ts"
      provides: "reasoningEffort field in SettingsSlice"
      contains: "reasoningEffort"
  key_links:
    - from: "src/services/llm/providers/OpenAIProvider.ts"
      to: "src/services/llm/providers/LLMProvider.ts"
      via: "imports ReasoningEffort type and isReasoningModel"
      pattern: "import.*isReasoningModel.*LLMProvider"
    - from: "src/store/index.ts"
      to: "src/store/types.ts"
      via: "partialize includes reasoningEffort"
      pattern: "reasoningEffort"
---

<objective>
Update the LLM provider layer and store to support reasoning models (o-series and GPT-5) with correct API parameters, updated model lists, and a new reasoningEffort store setting.

Purpose: This is the foundation for Phase 16 -- without correct API parameter handling, reasoning models will return empty responses (token budget too low) or API errors (unsupported parameters). Without the store setting, the UI cannot control reasoning effort.

Output: Updated provider interfaces, model lists, request body construction, and store types that enable reasoning model support.
</objective>

<execution_context>
@/Users/sasha-marchuk/.claude/get-shit-done/workflows/execute-plan.md
@/Users/sasha-marchuk/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/16-reasoning-models/16-RESEARCH.md

@src/services/llm/providers/LLMProvider.ts
@src/services/llm/providers/OpenAIProvider.ts
@src/services/llm/providers/OpenRouterProvider.ts
@src/services/llm/providers/streamSSE.ts
@src/services/llm/types.ts
@src/services/llm/index.ts
@src/store/types.ts
@src/store/settingsSlice.ts
@src/store/index.ts
</context>

<tasks>

<task type="auto">
  <name>Task 1: Extend provider types, add reasoning utility, update model lists, add store setting</name>
  <files>
    src/services/llm/providers/LLMProvider.ts
    src/services/llm/providers/OpenAIProvider.ts
    src/services/llm/providers/OpenRouterProvider.ts
    src/services/llm/index.ts
    src/store/types.ts
    src/store/settingsSlice.ts
    src/store/index.ts
  </files>
  <action>
    **LLMProvider.ts:**
    1. Add `ReasoningEffort` type: `export type ReasoningEffort = 'low' | 'medium' | 'high';`
    2. Add `isReasoningModel` utility function (exported). Logic:
       - Strip provider prefix (e.g., "openai/o4-mini" -> "o4-mini")
       - Match o-series prefixes: `['o1', 'o3', 'o4']` (bareModel === prefix OR bareModel.startsWith(`${prefix}-`))
       - Match GPT-5 series: `['gpt-5', 'gpt-5-mini', 'gpt-5-nano']` (bareModel === model OR bareModel.startsWith(`${model}-`))
       - Return boolean
    3. Add `reasoningEffort?: ReasoningEffort` optional field to `ProviderStreamOptions` interface
    4. Add `MIN_REASONING_TOKEN_BUDGET = 25_000` as exported constant

    **OpenAIProvider.ts:**
    1. Remove the local `OPENAI_REASONING_MODEL_PREFIXES` array and local `isReasoningModel` function
    2. Import `isReasoningModel`, `ReasoningEffort`, `MIN_REASONING_TOKEN_BUDGET` from `./LLMProvider`
    3. Update `OPENAI_MODELS` array:
       - ADD: `{ id: 'gpt-5', name: 'GPT-5', category: 'full', provider: 'openai' }`
       - ADD: `{ id: 'gpt-5-mini', name: 'GPT-5 Mini', category: 'fast', provider: 'openai' }`
       - ADD: `{ id: 'gpt-5-nano', name: 'GPT-5 Nano', category: 'fast', provider: 'openai' }`
       - ADD: `{ id: 'o4-mini', name: 'o4 Mini', category: 'fast', provider: 'openai' }`
       - REMOVE: `{ id: 'o1-preview', ... }` (deprecated)
       - REMOVE: `{ id: 'gpt-3.5-turbo', ... }` (legacy)
       - REMOVE: `{ id: 'gpt-4-turbo', ... }` (superseded)
       - REMOVE: `{ id: 'gpt-4', ... }` (superseded)
       - Keep all other existing models (gpt-4o-mini, gpt-4.1-mini, gpt-4.1-nano, gpt-4o, gpt-4.1, o1, o1-mini, o3-mini)
    4. Update `streamResponse()` -- will be done in Task 2

    **OpenRouterProvider.ts:**
    1. Remove the local `REASONING_MODEL_PREFIXES` array and local `isReasoningModel` function
    2. Import `isReasoningModel`, `MIN_REASONING_TOKEN_BUDGET` from `./LLMProvider`
    3. Add reasoning models to `OPENROUTER_MODELS`:
       - ADD: `{ id: 'openai/o3-mini', name: 'o3 Mini', category: 'fast', provider: 'openrouter' }`
       - ADD: `{ id: 'openai/o4-mini', name: 'o4 Mini', category: 'fast', provider: 'openrouter' }`
       - ADD: `{ id: 'openai/gpt-5', name: 'GPT-5', category: 'full', provider: 'openrouter' }`
       - ADD: `{ id: 'openai/gpt-5-mini', name: 'GPT-5 Mini', category: 'fast', provider: 'openrouter' }`
       Keep existing models unchanged

    **llm/index.ts:**
    Add exports for new public types:
    - Export `ReasoningEffort` type from `./providers/LLMProvider`
    - Export `isReasoningModel` function from `./providers/LLMProvider`
    - Export `MIN_REASONING_TOKEN_BUDGET` from `./providers/LLMProvider`

    **store/types.ts:**
    1. Import `ReasoningEffort` from the llm service: No -- store types should NOT depend on services. Instead, define the type independently here:
       `export type ReasoningEffort = 'low' | 'medium' | 'high';`
       (The LLMProvider.ts also defines it, both are the same 3-value union. This avoids circular dependency. The background.ts bridges them.)
    2. Add to `SettingsSlice` interface:
       - `reasoningEffort: ReasoningEffort;` (state field)
       - `setReasoningEffort: (effort: ReasoningEffort) => void;` (action)

    **settingsSlice.ts:**
    1. Import `ReasoningEffort` from `./types`
    2. Add `reasoningEffort: 'medium' as ReasoningEffort` to `DEFAULT_SETTINGS`
    3. Add state: `reasoningEffort: DEFAULT_SETTINGS.reasoningEffort`
    4. Add action: `setReasoningEffort: (effort: ReasoningEffort) => { set(() => ({ reasoningEffort: effort })); }`

    **store/index.ts:**
    1. Add `reasoningEffort: state.reasoningEffort` to the `partialize` function
    2. Add `ReasoningEffort` to the re-exports from `./types`
  </action>
  <verify>
    Run `npx tsc --noEmit` -- should compile with zero errors.
    Run `npx eslint src/services/llm/providers/LLMProvider.ts src/services/llm/providers/OpenAIProvider.ts src/services/llm/providers/OpenRouterProvider.ts src/store/types.ts src/store/settingsSlice.ts src/store/index.ts` -- should have zero lint errors.
  </verify>
  <done>
    - `ReasoningEffort` type exists in both LLMProvider.ts and store/types.ts
    - `isReasoningModel()` exported from LLMProvider.ts correctly identifies o1, o1-mini, o3, o3-mini, o4-mini, gpt-5, gpt-5-mini, gpt-5-nano as reasoning models, and returns false for gpt-4o, gpt-4.1, etc.
    - `ProviderStreamOptions` has optional `reasoningEffort` field
    - `OPENAI_MODELS` contains o4-mini, gpt-5, gpt-5-mini, gpt-5-nano; does NOT contain o1-preview, gpt-3.5-turbo, gpt-4-turbo, gpt-4
    - `OPENROUTER_MODELS` contains openai/o3-mini, openai/o4-mini, openai/gpt-5, openai/gpt-5-mini
    - Store has `reasoningEffort` field persisted in partialize
    - TypeScript compiles without errors
  </done>
</task>

<task type="auto">
  <name>Task 2: Update provider request body construction for reasoning models</name>
  <files>
    src/services/llm/providers/OpenAIProvider.ts
    src/services/llm/providers/OpenRouterProvider.ts
    src/services/llm/providers/streamSSE.ts
  </files>
  <action>
    **OpenAIProvider.ts -- update `streamResponse()`:**
    1. Import `MIN_REASONING_TOKEN_BUDGET` from `./LLMProvider`
    2. Extract reasoning status: `const reasoning = isReasoningModel(model);`
    3. Determine message role: Use `'developer'` for reasoning models, `'system'` for standard models
       ```
       const messages = [
         { role: reasoning ? 'developer' : 'system', content: systemPrompt },
         { role: 'user', content: userPrompt },
       ];
       ```
    4. Enforce minimum token budget for reasoning models:
       ```
       const tokenBudget = reasoning
         ? Math.max(maxTokens, MIN_REASONING_TOKEN_BUDGET)
         : maxTokens;
       ```
    5. Use correct token parameter name:
       ```
       const tokenLimit = reasoning
         ? { max_completion_tokens: tokenBudget }
         : { max_tokens: tokenBudget };
       ```
       Note: for standard models, keep using `maxTokens` directly (not `tokenBudget`), i.e., `{ max_tokens: maxTokens }`.
    6. Build request body WITHOUT temperature/top_p for reasoning models:
       ```
       const body: Record<string, unknown> = {
         model,
         messages,
         ...tokenLimit,
         stream: true,
       };
       if (reasoning && options.reasoningEffort) {
         body.reasoning_effort = options.reasoningEffort;
       }
       ```
    7. Pass the body to `streamSSE` (same as before, body is already `Record<string, unknown>`)

    **OpenRouterProvider.ts -- update `streamResponse()`:**
    Apply the SAME changes as OpenAIProvider.ts:
    1. Import `MIN_REASONING_TOKEN_BUDGET` from `./LLMProvider`
    2. Use `'developer'` role for reasoning models
    3. Enforce 25K min token budget for reasoning models
    4. Use `max_completion_tokens` for reasoning, `max_tokens` for standard
    5. Add `reasoning_effort` when specified for reasoning models
    6. Keep existing OpenRouter-specific headers (HTTP-Referer, X-Title) and `checkErrorFinishReason: true`

    **streamSSE.ts -- add non-streaming fallback:**
    The current `streamSSE` always expects a streaming response. Add a try/catch that detects streaming failures for reasoning models and falls back to a non-streaming request:

    1. After the `fetch` call succeeds and `response.ok` is true, check if the response has a body. If it does, proceed as normal (streaming).
    2. If during stream reading we get a specific error pattern indicating streaming is not supported (e.g., response content-type is `application/json` instead of `text/event-stream`), fall back to non-streaming:
       - Check `response.headers.get('content-type')` right after fetch succeeds
       - If content-type includes `application/json` (not `text/event-stream`), treat as non-streaming response:
         - Read full body as JSON
         - Extract content from `response.choices[0].message.content`
         - Call `onToken(content)` then `onComplete()`
         - Return early (skip SSE parsing)
    3. If content-type is `text/event-stream` or includes it, proceed with existing SSE parsing logic unchanged
    4. This is a graceful fallback -- no new error types needed, just detection and alternative processing path

    Important: Do NOT change the function signature. The fallback is transparent to callers.
  </action>
  <verify>
    Run `npx tsc --noEmit` -- should compile with zero errors.
    Run `npx eslint src/services/llm/providers/OpenAIProvider.ts src/services/llm/providers/OpenRouterProvider.ts src/services/llm/providers/streamSSE.ts` -- should have zero lint errors.
    Verify by reading OpenAIProvider.ts that:
    - `system` role is NOT used when model is a reasoning model
    - `max_tokens` is NOT used when model is a reasoning model
    - `reasoning_effort` is included when model is reasoning AND options.reasoningEffort is provided
    - Token budget is >= 25000 when model is a reasoning model
  </verify>
  <done>
    - Both OpenAIProvider and OpenRouterProvider use `developer` role for reasoning models, `system` for standard
    - Both providers enforce MIN_REASONING_TOKEN_BUDGET (25K) for reasoning models
    - Both providers include `reasoning_effort` in request body when applicable
    - Both providers use `max_completion_tokens` for reasoning models, `max_tokens` for standard
    - streamSSE handles non-streaming JSON responses gracefully (fallback for models that don't support streaming)
    - TypeScript compiles without errors
  </done>
</task>

</tasks>

<verification>
1. `npx tsc --noEmit` passes
2. `npx eslint .` passes (or at minimum, no NEW errors in modified files)
3. Manual review confirms: reasoning models in model lists, developer role, 25K budget, reasoning_effort, no temperature/top_p
</verification>

<success_criteria>
- isReasoningModel identifies all o-series and GPT-5 models correctly
- Provider request bodies are correctly constructed for both reasoning and standard models
- Model lists contain all new models and exclude deprecated ones
- Store persists reasoningEffort setting
- Non-streaming fallback handles JSON responses from models that don't support streaming
- Zero TypeScript errors
</success_criteria>

<output>
After completion, create `.planning/phases/16-reasoning-models/16-01-SUMMARY.md`
</output>
