---
phase: 16-reasoning-models
plan: 02
type: execute
wave: 2
depends_on: ["16-01"]
files_modified:
  - src/types/messages.ts
  - src/services/llm/types.ts
  - entrypoints/background.ts
autonomous: true

must_haves:
  truths:
    - "LLM_REQUEST message can optionally carry isReasoningRequest flag and reasoningEffort level"
    - "When reasoning mode is requested, background fires a SINGLE request (not dual fast+full)"
    - "Reasoning requests use the full model with minimum 25K token budget"
    - "reasoningEffort from store is passed through to the provider streamResponse call"
  artifacts:
    - path: "src/types/messages.ts"
      provides: "Extended LLMRequestMessage with optional reasoning fields"
      contains: "isReasoningRequest"
    - path: "entrypoints/background.ts"
      provides: "handleLLMRequest supporting reasoning mode (single-stream)"
      contains: "isReasoningRequest"
  key_links:
    - from: "entrypoints/background.ts"
      to: "src/services/llm/providers/LLMProvider.ts"
      via: "passes reasoningEffort to provider.streamResponse options"
      pattern: "reasoningEffort"
    - from: "entrypoints/background.ts"
      to: "src/types/messages.ts"
      via: "reads isReasoningRequest from LLMRequestMessage"
      pattern: "isReasoningRequest"
---

<objective>
Update message types and background handler to support reasoning model requests as a single-stream operation (not dual fast+full) with reasoning_effort passthrough and 25K minimum token budget enforcement.

Purpose: The existing dual-stream architecture fires both fast (300 tokens) and full (2000 tokens) requests in parallel. Reasoning models need 25K+ tokens and are expensive -- firing TWO reasoning requests makes no sense. The reasoning button must fire a SINGLE request to the full model only. This plan implements that routing.

Output: Updated message types and background handler that correctly route reasoning requests as single-stream with proper token budgets.
</objective>

<execution_context>
@/Users/sasha-marchuk/.claude/get-shit-done/workflows/execute-plan.md
@/Users/sasha-marchuk/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/16-reasoning-models/16-RESEARCH.md
@.planning/phases/16-reasoning-models/16-01-SUMMARY.md

@src/types/messages.ts
@src/services/llm/types.ts
@entrypoints/background.ts
@src/services/llm/providers/LLMProvider.ts
</context>

<tasks>

<task type="auto">
  <name>Task 1: Extend message types and LLM types for reasoning requests</name>
  <files>
    src/types/messages.ts
    src/services/llm/types.ts
  </files>
  <action>
    **messages.ts:**
    1. Add optional reasoning fields to `LLMRequestMessage`:
       ```typescript
       export interface LLMRequestMessage extends BaseMessage {
         type: 'LLM_REQUEST';
         responseId: string;
         question: string;
         recentContext: string;
         fullTranscript: string;
         templateId: string;
         /** Optional: if true, this is a single-stream reasoning request (not dual fast+full) */
         isReasoningRequest?: boolean;
         /** Optional: reasoning effort level for reasoning models */
         reasoningEffort?: 'low' | 'medium' | 'high';
       }
       ```
    2. No changes to MessageType union (still 'LLM_REQUEST' -- same type, extended fields)
    3. No changes to ExtensionMessage union (LLMRequestMessage is already included)

    **llm/types.ts:**
    1. Add optional reasoning fields to `DualLLMRequest`:
       ```typescript
       export interface DualLLMRequest {
         question: string;
         recentContext: string;
         fullTranscript: string;
         templateId: string;
         /** Optional: if true, this is a reasoning request (single-stream, not dual) */
         isReasoningRequest?: boolean;
         /** Optional: reasoning effort level */
         reasoningEffort?: 'low' | 'medium' | 'high';
       }
       ```
    2. Update `OpenRouterChatMessage` role type to include `'developer'`:
       ```typescript
       export interface OpenRouterChatMessage {
         role: 'system' | 'user' | 'assistant' | 'developer';
         content: string;
       }
       ```
  </action>
  <verify>
    Run `npx tsc --noEmit` -- should compile without errors.
  </verify>
  <done>
    - `LLMRequestMessage` has optional `isReasoningRequest` and `reasoningEffort` fields
    - `DualLLMRequest` has optional `isReasoningRequest` and `reasoningEffort` fields
    - `OpenRouterChatMessage` role includes `'developer'`
    - No breaking changes to existing message handling
  </done>
</task>

<task type="auto">
  <name>Task 2: Update background handler for single-stream reasoning requests</name>
  <files>entrypoints/background.ts</files>
  <action>
    **Modify `handleLLMRequest` function signature and body:**

    1. Add two new parameters: `isReasoningRequest?: boolean`, `reasoningEffort?: string`
       New signature:
       ```typescript
       async function handleLLMRequest(
         responseId: string,
         question: string,
         recentContext: string,
         fullTranscript: string,
         templateId: string,
         isReasoningRequest?: boolean,
         reasoningEffort?: string,
       ): Promise<void>
       ```

    2. If `isReasoningRequest` is true, use SINGLE-STREAM mode:
       - Only fire the FULL model request (skip fast model entirely)
       - Set `fastComplete = true` immediately (skip fast stream)
       - For the full model request, override `maxTokens` to enforce minimum 25K:
         ```typescript
         const fullMaxTokens = isReasoningRequest
           ? Math.max(2000, MIN_REASONING_TOKEN_BUDGET)  // At least 25K for reasoning
           : 2000;
         ```
       - Import `MIN_REASONING_TOKEN_BUDGET` from `../src/services/llm/providers/LLMProvider`

    3. Pass `reasoningEffort` to the provider via `streamWithRetry`:
       - Extend `StreamWithRetryParams` interface with `reasoningEffort?: string`
       - In `streamWithRetry`, pass it to `provider.streamResponse()`:
         ```typescript
         await params.provider.streamResponse({
           ...existingOptions,
           reasoningEffort: params.reasoningEffort as ReasoningEffort | undefined,
         });
         ```
       - Import `ReasoningEffort` type from `../src/services/llm/providers/LLMProvider`

    4. In the `fireModelRequest` function, pass `reasoningEffort` through:
       - Add `reasoningEffort?: string` to the `ModelRequestConfig` interface
       - Pass it to `streamWithRetry` params

    5. When firing the full model in reasoning mode, pass `reasoningEffort`:
       ```typescript
       const fullPromise = fireModelRequest({
         resolution: fullResolution,
         modelType: 'full',
         modelId: models.fullModel,
         userPrompt: prompts.userFull,
         maxTokens: isReasoningRequest ? Math.max(2000, MIN_REASONING_TOKEN_BUDGET) : 2000,
         reasoningEffort: isReasoningRequest ? reasoningEffort : undefined,
         onDone: () => { fullComplete = true; },
       });
       ```

    6. For the FAST model in reasoning mode, skip entirely:
       ```typescript
       if (isReasoningRequest) {
         // Reasoning mode: skip fast model, only fire full model
         fastComplete = true;
         // Send fast model status as complete immediately (nothing to show)
         await sendLLMMessageToMeet({
           type: 'LLM_STATUS',
           responseId,
           model: 'fast',
           status: 'complete',
         });
       } else {
         // Normal dual-stream: fire fast model
         const fastPromise = fireModelRequest({ ... });
       }
       ```

    7. Update the LLM_REQUEST case in the message handler switch to pass new fields:
       ```typescript
       case 'LLM_REQUEST': {
         // ... existing code ...
         handleLLMRequest(
           msg.responseId,
           msg.question,
           msg.recentContext,
           msg.fullTranscript,
           msg.templateId,
           msg.isReasoningRequest,
           msg.reasoningEffort,
         );
         break;
       }
       ```

    **Important considerations:**
    - The `reasoningEffort` from the message is a string. The provider expects `ReasoningEffort` type. Cast in `streamWithRetry` with a type assertion (safe because UI constrains to valid values).
    - The existing dual-stream behavior is UNCHANGED when `isReasoningRequest` is falsy. This is backwards-compatible.
    - Do NOT change the existing `maxTokens: 300` for fast model in non-reasoning mode. That stays as-is.
    - The provider (Plan 01) already handles the 25K minimum internally via `MIN_REASONING_TOKEN_BUDGET`. But the background should ALSO enforce it to be explicit about intent. Both layers enforce = defense in depth.
  </action>
  <verify>
    Run `npx tsc --noEmit` -- should compile without errors.
    Run `npx eslint entrypoints/background.ts` -- should have zero lint errors.
    Read the modified handleLLMRequest and verify:
    1. When isReasoningRequest=true: only full model fires, fast is skipped
    2. reasoningEffort is passed through to streamWithRetry and then to provider.streamResponse
    3. maxTokens for reasoning is >= 25000
    4. When isReasoningRequest is falsy: existing dual-stream behavior is unchanged
  </verify>
  <done>
    - handleLLMRequest accepts optional isReasoningRequest and reasoningEffort parameters
    - Reasoning requests fire single-stream (full model only)
    - Fast model is skipped in reasoning mode with immediate complete status
    - reasoningEffort flows from message -> handleLLMRequest -> fireModelRequest -> streamWithRetry -> provider.streamResponse
    - Token budget is >= 25K for reasoning requests
    - Non-reasoning requests behave identically to before (backwards compatible)
    - LLM_REQUEST message handler passes new fields
    - TypeScript compiles without errors
  </done>
</task>

</tasks>

<verification>
1. `npx tsc --noEmit` passes
2. `npx eslint entrypoints/background.ts src/types/messages.ts src/services/llm/types.ts` passes
3. Code review confirms: reasoning mode = single stream, 25K budget, reasoningEffort passthrough
4. Code review confirms: non-reasoning mode unchanged (dual stream, 300/2000 tokens)
</verification>

<success_criteria>
- LLMRequestMessage has isReasoningRequest and reasoningEffort optional fields
- Background handler routes reasoning requests as single-stream (full model only)
- reasoningEffort passes through entire chain to provider.streamResponse
- Non-reasoning behavior unchanged (backwards compatible)
- Zero TypeScript errors
</success_criteria>

<output>
After completion, create `.planning/phases/16-reasoning-models/16-02-SUMMARY.md`
</output>
