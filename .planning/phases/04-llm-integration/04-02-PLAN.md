---
phase: 04-llm-integration
plan: 02
type: execute
wave: 2
depends_on: ["04-01"]
files_modified:
  - src/types/messages.ts
  - entrypoints/background.ts
autonomous: true

must_haves:
  truths:
    - "LLM_REQUEST message triggers dual parallel streaming"
    - "Fast and full responses stream independently to content script"
    - "Streaming can be cancelled via abort"
    - "Service Worker stays alive during long streaming responses"
  artifacts:
    - path: "src/types/messages.ts"
      provides: "LLM message types for request/response flow"
      contains: "LLM_REQUEST"
    - path: "entrypoints/background.ts"
      provides: "LLM request handler with dual parallel streaming"
      contains: "sendDualLLMRequests"
  key_links:
    - from: "entrypoints/background.ts"
      to: "src/services/llm"
      via: "import streamLLMResponse, buildPrompt"
      pattern: "streamLLMResponse"
    - from: "entrypoints/background.ts"
      to: "src/store"
      via: "import store for settings/templates"
      pattern: "useStore\\.getState"
---

<objective>
Add LLM message types and implement dual parallel streaming in the Service Worker background.ts.

Purpose: Service Worker handles LLM_REQUEST from content script, fires dual parallel requests to OpenRouter, and streams tokens back via LLM_STREAM messages.
Output: Complete message-based LLM flow from request to streaming response.
</objective>

<execution_context>
@/Users/sasha-marchuk/.claude/get-shit-done/workflows/execute-plan.md
@/Users/sasha-marchuk/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/04-llm-integration/04-RESEARCH.md
@src/types/messages.ts
@entrypoints/background.ts
@src/store/index.ts
@src/store/settingsSlice.ts
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add LLM message types</name>
  <files>src/types/messages.ts</files>
  <action>
Add LLM-related message types to src/types/messages.ts:

1. Add to MessageType union:
   - 'LLM_REQUEST'
   - 'LLM_STREAM'
   - 'LLM_STATUS'
   - 'LLM_CANCEL'

2. Add interfaces:

```typescript
// LLM request from content script to background
export interface LLMRequestMessage extends BaseMessage {
  type: 'LLM_REQUEST';
  responseId: string;        // Unique ID for this request/response pair
  question: string;          // Captured/highlighted text
  recentContext: string;     // Last N transcript entries formatted
  fullTranscript: string;    // Full session transcript formatted
  templateId: string;        // Active template ID
}

// Streaming token updates from background to content script
export interface LLMStreamMessage extends BaseMessage {
  type: 'LLM_STREAM';
  responseId: string;
  model: 'fast' | 'full';
  token: string;
}

// Status updates for the LLM request lifecycle
export interface LLMStatusMessage extends BaseMessage {
  type: 'LLM_STATUS';
  responseId: string;
  model: 'fast' | 'full' | 'both';
  status: 'pending' | 'streaming' | 'complete' | 'error';
  error?: string;
}

// Cancel an in-flight LLM request
export interface LLMCancelMessage extends BaseMessage {
  type: 'LLM_CANCEL';
  responseId: string;
}
```

3. Add all four to ExtensionMessage union type

4. Update the exhaustive switch check in background.ts will need these cases (done in Task 2)
  </action>
  <verify>
   - `npx tsc --noEmit src/types/messages.ts` passes
   - All new types export correctly
  </verify>
  <done>LLM message types defined for complete request/stream/status/cancel lifecycle</done>
</task>

<task type="auto">
  <name>Task 2: Implement dual parallel LLM streaming in background.ts</name>
  <files>entrypoints/background.ts</files>
  <action>
Update entrypoints/background.ts to handle LLM requests:

1. Add imports at top:
   ```typescript
   import { streamLLMResponse, buildPrompt, type DualLLMRequest } from '../src/services/llm';
   import type {
     LLMRequestMessage,
     LLMStreamMessage,
     LLMStatusMessage,
     LLMCancelMessage,
   } from '../src/types/messages';
   import { useStore } from '../src/store';
   ```

2. Add module-level state for LLM request tracking:
   ```typescript
   // Track active LLM requests for cancellation
   let activeAbortControllers: Map<string, AbortController> = new Map();

   // Keep service worker alive during streaming
   let keepAliveInterval: ReturnType<typeof setInterval> | null = null;

   function startKeepAlive() {
     if (keepAliveInterval) return;
     keepAliveInterval = setInterval(() => {
       chrome.runtime.getPlatformInfo(() => {});
     }, 20000);
   }

   function stopKeepAlive() {
     if (keepAliveInterval) {
       clearInterval(keepAliveInterval);
       keepAliveInterval = null;
     }
   }
   ```

3. Add helper to send LLM messages to content script:
   ```typescript
   async function sendLLMMessageToMeet(message: LLMStreamMessage | LLMStatusMessage): Promise<void> {
     const tabs = await chrome.tabs.query({ url: 'https://meet.google.com/*' });
     for (const tab of tabs) {
       if (tab.id) {
         chrome.tabs.sendMessage(tab.id, message).catch(() => {});
       }
     }
   }
   ```

4. Add the dual LLM request handler function:
   ```typescript
   async function handleLLMRequest(message: LLMRequestMessage): Promise<{ success: boolean; error?: string }> {
     const { responseId, question, recentContext, fullTranscript, templateId } = message;

     // Get settings from store
     const state = useStore.getState();
     const { openRouter: apiKey } = state.apiKeys;
     const { fastModel, fullModel } = state.models;

     if (!apiKey) {
       return { success: false, error: 'OpenRouter API key not configured' };
     }

     // Find active template
     const template = state.templates.find(t => t.id === templateId);
     if (!template) {
       return { success: false, error: 'Template not found' };
     }

     // Build prompts
     const request: DualLLMRequest = { question, recentContext, fullTranscript, templateId };
     const prompts = buildPrompt(request, template);

     // Create abort controller for cancellation
     const controller = new AbortController();
     activeAbortControllers.set(responseId, controller);

     // Start keep-alive
     startKeepAlive();

     // Send initial pending status
     await sendLLMMessageToMeet({
       type: 'LLM_STATUS',
       responseId,
       model: 'both',
       status: 'pending',
     });

     // Track completion state
     let fastComplete = false;
     let fullComplete = false;

     const checkAllComplete = () => {
       if (fastComplete && fullComplete) {
         activeAbortControllers.delete(responseId);
         if (activeAbortControllers.size === 0) {
           stopKeepAlive();
         }
       }
     };

     // Use model override from template if present
     const actualFastModel = template.modelOverride || fastModel;
     const actualFullModel = template.modelOverride || fullModel;

     // Fire BOTH requests in parallel
     const fastPromise = streamLLMResponse({
       model: actualFastModel,
       systemPrompt: prompts.system,
       userPrompt: prompts.user,
       maxTokens: 100, // Short hint
       apiKey,
       abortSignal: controller.signal,
       onToken: async (token) => {
         await sendLLMMessageToMeet({
           type: 'LLM_STREAM',
           responseId,
           model: 'fast',
           token,
         });
       },
       onComplete: async () => {
         fastComplete = true;
         await sendLLMMessageToMeet({
           type: 'LLM_STATUS',
           responseId,
           model: 'fast',
           status: 'complete',
         });
         checkAllComplete();
       },
       onError: async (error) => {
         fastComplete = true;
         await sendLLMMessageToMeet({
           type: 'LLM_STATUS',
           responseId,
           model: 'fast',
           status: 'error',
           error,
         });
         checkAllComplete();
       },
     }).catch((e) => {
       fastComplete = true;
       sendLLMMessageToMeet({
         type: 'LLM_STATUS',
         responseId,
         model: 'fast',
         status: 'error',
         error: e.message || 'Fast model request failed',
       });
       checkAllComplete();
     });

     const fullPromise = streamLLMResponse({
       model: actualFullModel,
       systemPrompt: prompts.system,
       userPrompt: prompts.userFull,
       maxTokens: 1000, // Comprehensive answer
       apiKey,
       abortSignal: controller.signal,
       onToken: async (token) => {
         await sendLLMMessageToMeet({
           type: 'LLM_STREAM',
           responseId,
           model: 'full',
           token,
         });
       },
       onComplete: async () => {
         fullComplete = true;
         await sendLLMMessageToMeet({
           type: 'LLM_STATUS',
           responseId,
           model: 'full',
           status: 'complete',
         });
         checkAllComplete();
       },
       onError: async (error) => {
         fullComplete = true;
         await sendLLMMessageToMeet({
           type: 'LLM_STATUS',
           responseId,
           model: 'full',
           status: 'error',
           error,
         });
         checkAllComplete();
       },
     }).catch((e) => {
       fullComplete = true;
       sendLLMMessageToMeet({
         type: 'LLM_STATUS',
         responseId,
         model: 'full',
         status: 'error',
         error: e.message || 'Full model request failed',
       });
       checkAllComplete();
     });

     // Don't await - let them stream independently
     Promise.allSettled([fastPromise, fullPromise]);

     return { success: true };
   }
   ```

5. Add switch cases in handleMessage for the new message types:
   ```typescript
   case 'LLM_REQUEST':
     return handleLLMRequest(message);

   case 'LLM_STREAM':
   case 'LLM_STATUS':
     // These are outbound only (background -> content), shouldn't be received here
     console.log('LLM message received in background (outbound only):', message.type);
     return { received: true };

   case 'LLM_CANCEL': {
     const controller = activeAbortControllers.get(message.responseId);
     if (controller) {
       controller.abort();
       activeAbortControllers.delete(message.responseId);
       if (activeAbortControllers.size === 0) {
         stopKeepAlive();
       }
     }
     return { cancelled: true };
   }
   ```

Note: The existing exhaustive default case will catch any missed message types.
  </action>
  <verify>
   - `npm run build` passes
   - No TypeScript errors in background.ts
   - All LLM message cases handled in switch
  </verify>
  <done>Service Worker handles LLM_REQUEST with dual parallel streaming, proper cancellation, and keep-alive</done>
</task>

</tasks>

<verification>
After all tasks complete:
1. `npm run build` passes without errors
2. All 4 new message types in ExtensionMessage union
3. background.ts imports from src/services/llm
4. handleLLMRequest fires two parallel streams
5. AbortController enables request cancellation
</verification>

<success_criteria>
- LLM message types fully defined (REQUEST, STREAM, STATUS, CANCEL)
- Service Worker receives LLM_REQUEST and fires dual parallel streams
- Tokens stream back via LLM_STREAM messages to content script
- Service Worker stays alive during long responses via keepAlive
- Cancellation works via LLM_CANCEL and AbortController
</success_criteria>

<output>
After completion, create `.planning/phases/04-llm-integration/04-02-SUMMARY.md`
</output>
