---
phase: 04-llm-integration
plan: 04
type: execute
wave: 3
depends_on: ["04-02", "04-03"]
files_modified:
  - entrypoints/content.tsx
  - src/overlay/Overlay.tsx
  - src/overlay/CaptureIndicator.tsx
autonomous: false

must_haves:
  truths:
    - "LLM streaming tokens update overlay in real-time"
    - "Fast hint and full answer display simultaneously"
    - "Visual indicator shows when capture mode is active"
    - "Complete flow works: hold hotkey -> release -> see responses"
  artifacts:
    - path: "src/overlay/CaptureIndicator.tsx"
      provides: "Visual indicator component for capture state"
    - path: "src/overlay/Overlay.tsx"
      provides: "Updated overlay with real LLM response state"
      contains: "LLMResponse state management"
    - path: "entrypoints/content.tsx"
      provides: "LLM_STREAM message handling"
      contains: "LLM_STREAM"
  key_links:
    - from: "entrypoints/content.tsx"
      to: "src/overlay/Overlay.tsx"
      via: "llm-response-update custom event"
      pattern: "dispatchEvent"
    - from: "src/overlay/Overlay.tsx"
      to: "src/overlay/CaptureIndicator.tsx"
      via: "component import"
      pattern: "CaptureIndicator"
---

<objective>
Wire LLM streaming responses to overlay, add visual capture indicator, and verify complete end-to-end flow.

Purpose: Complete the LLM integration by displaying streaming responses in the UI and providing visual feedback during capture.
Output: Working overlay with real-time LLM responses and capture state indication.
</objective>

<execution_context>
@/Users/sasha-marchuk/.claude/get-shit-done/workflows/execute-plan.md
@/Users/sasha-marchuk/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/04-llm-integration/04-RESEARCH.md
@entrypoints/content.tsx
@src/overlay/Overlay.tsx
@src/overlay/ResponsePanel.tsx
@src/types/transcript.ts
</context>

<tasks>

<task type="auto">
  <name>Task 1: Handle LLM messages in content script and dispatch to overlay</name>
  <files>entrypoints/content.tsx</files>
  <action>
Update entrypoints/content.tsx to handle LLM_STREAM and LLM_STATUS messages:

1. Add imports for LLM message types:
```typescript
import type {
  LLMStreamMessage,
  LLMStatusMessage,
} from '../src/types/messages';
import type { LLMResponse } from '../src/types/transcript';
```

2. Add module-level LLM response state:
```typescript
// Module-level LLM response state
let currentResponse: LLMResponse | null = null;

/**
 * Dispatch LLM response update to the React overlay via custom event
 */
function dispatchLLMResponseUpdate(response: LLMResponse | null): void {
  window.dispatchEvent(
    new CustomEvent<{ response: LLMResponse | null }>('llm-response-update', {
      detail: { response },
    })
  );
}

/**
 * Initialize a new LLM response
 */
function initLLMResponse(responseId: string): void {
  currentResponse = {
    id: responseId,
    questionId: responseId, // Same as response ID for now
    fastHint: null,
    fullAnswer: null,
    status: 'pending',
  };
  dispatchLLMResponseUpdate(currentResponse);
}

/**
 * Update LLM response with streaming token
 */
function handleLLMStream(message: LLMStreamMessage): void {
  if (!currentResponse || currentResponse.id !== message.responseId) {
    // Initialize if we receive stream before status
    initLLMResponse(message.responseId);
  }

  if (currentResponse) {
    if (message.model === 'fast') {
      currentResponse.fastHint = (currentResponse.fastHint || '') + message.token;
    } else {
      currentResponse.fullAnswer = (currentResponse.fullAnswer || '') + message.token;
    }
    currentResponse.status = 'streaming';
    dispatchLLMResponseUpdate({ ...currentResponse });
  }
}

/**
 * Update LLM response status
 */
function handleLLMStatus(message: LLMStatusMessage): void {
  if (!currentResponse || currentResponse.id !== message.responseId) {
    // Initialize if this is first message
    initLLMResponse(message.responseId);
  }

  if (currentResponse) {
    // Update status based on message
    if (message.status === 'error') {
      currentResponse.error = message.error;
      currentResponse.status = 'error';
    } else if (message.status === 'complete' && message.model === 'both') {
      // Only set complete when both are done (won't happen with current impl)
      currentResponse.status = 'complete';
    } else if (message.status === 'complete') {
      // Individual model complete - check if both done
      // For now, set to streaming until both complete
      // The UI will show streaming indicator until all content stops
      currentResponse.status = 'streaming';
    } else {
      currentResponse.status = message.status;
    }
    dispatchLLMResponseUpdate({ ...currentResponse });
  }
}
```

3. Update the message listener to handle LLM messages:
```typescript
// Inside chrome.runtime.onMessage.addListener callback
if (message.type === 'LLM_STREAM') {
  handleLLMStream(message as LLMStreamMessage);
  return false;
}

if (message.type === 'LLM_STATUS') {
  handleLLMStatus(message as LLMStatusMessage);
  return false;
}
```

4. Update sendLLMRequest to initialize response state before sending:
```typescript
// In sendLLMRequest, after creating responseId
initLLMResponse(responseId);
```
  </action>
  <verify>
   - `npm run build` passes
   - LLM message handlers added to listener
   - Custom event dispatching works
  </verify>
  <done>Content script handles LLM_STREAM and LLM_STATUS, dispatches to overlay via custom event</done>
</task>

<task type="auto">
  <name>Task 2: Create CaptureIndicator component</name>
  <files>src/overlay/CaptureIndicator.tsx</files>
  <action>
Create `src/overlay/CaptureIndicator.tsx`:

```typescript
import type { CaptureState } from '../hooks';

interface CaptureIndicatorProps {
  captureState: CaptureState | null;
}

/**
 * Visual indicator showing capture mode state.
 * Displays prominently when user is holding the capture hotkey.
 */
export function CaptureIndicator({ captureState }: CaptureIndicatorProps) {
  if (!captureState?.isHolding) {
    return null;
  }

  return (
    <div className="absolute top-0 left-0 right-0 bg-gradient-to-r from-red-500/80 to-orange-500/80 text-white text-center py-2 px-4 text-sm font-medium animate-pulse z-10 rounded-t-lg">
      <div className="flex items-center justify-center gap-2">
        <span className="w-3 h-3 bg-white rounded-full animate-ping"></span>
        <span>Capturing question... Release to send</span>
      </div>
    </div>
  );
}
```

This component:
- Only renders when isHolding is true
- Shows a prominent gradient bar at the top of the overlay
- Includes pulsing animation to draw attention
- Clear instruction text
  </action>
  <verify>
   - `npx tsc --noEmit src/overlay/CaptureIndicator.tsx` passes
   - Component renders conditionally based on captureState
  </verify>
  <done>CaptureIndicator component provides visual feedback during hotkey hold</done>
</task>

<task type="auto">
  <name>Task 3: Update Overlay to use real LLM response state and capture indicator</name>
  <files>src/overlay/Overlay.tsx, src/overlay/index.ts</files>
  <action>
Update `src/overlay/Overlay.tsx`:

1. Add imports:
```typescript
import { CaptureIndicator } from './CaptureIndicator';
import type { CaptureState } from '../hooks';
```

2. Add LLM response event listener alongside transcript listener:
```typescript
// Inside Overlay component, add state for real LLM response
const [llmResponse, setLLMResponse] = useState<LLMResponse | null>(null);

// Listen for LLM response updates from content script
useEffect(() => {
  function handleLLMUpdate(event: Event) {
    const customEvent = event as CustomEvent<{ response: LLMResponse | null }>;
    setLLMResponse(customEvent.detail.response);
  }

  window.addEventListener('llm-response-update', handleLLMUpdate);
  return () => {
    window.removeEventListener('llm-response-update', handleLLMUpdate);
  };
}, []);
```

3. Update props interface to accept captureState:
```typescript
interface OverlayProps {
  captureState?: CaptureState | null;
}
```

4. Update the component to use real LLM response:
```typescript
// Replace mockResponse logic
// Remove: const [mockResponse] = useState<LLMResponse | null>(MOCK_RESPONSE);
// Remove: const displayResponse = response ?? mockResponse;

// Use real LLM response state
const displayResponse = llmResponse;
```

5. Add CaptureIndicator to the overlay layout (inside the overlay container, before OverlayHeader):
```typescript
<div
  className="overlay-container h-full flex flex-col bg-black/10 rounded-lg shadow-2xl border border-white/20 overflow-hidden relative"
  style={{ backdropFilter: `blur(${blurLevel}px)` }}
>
  <CaptureIndicator captureState={captureState} />
  <OverlayHeader onMinimize={() => setMinimized(true)} />
  {/* ... rest of content */}
</div>
```

Note the added `relative` class for absolute positioning of CaptureIndicator.

6. Update the footer status to show LLM status:
```typescript
{/* Footer with status indicator */}
<div className="px-3 py-1.5 border-t border-white/10 flex items-center justify-between text-xs text-white/60">
  <span>AI Interview Assistant</span>
  <span className="flex items-center gap-1">
    {llmResponse?.status === 'streaming' ? (
      <>
        <span className="w-2 h-2 bg-blue-400 rounded-full animate-pulse"></span>
        Streaming...
      </>
    ) : llmResponse?.status === 'pending' ? (
      <>
        <span className="w-2 h-2 bg-yellow-400 rounded-full animate-pulse"></span>
        Processing...
      </>
    ) : (
      <>
        <span className="w-2 h-2 bg-green-400 rounded-full"></span>
        Ready
      </>
    )}
  </span>
</div>
```

7. Update `src/overlay/index.ts` to export CaptureIndicator:
```typescript
export { CaptureIndicator } from './CaptureIndicator';
```

8. Remove MOCK_RESPONSE import since we no longer use it (keep it in types for testing):
```typescript
// Remove from imports: MOCK_RESPONSE
import {
  type TranscriptEntry,
  type LLMResponse,
} from '../types/transcript';
```
  </action>
  <verify>
   - `npm run build` passes
   - Overlay uses llmResponse state
   - CaptureIndicator renders during capture
  </verify>
  <done>Overlay displays real LLM responses and capture indicator</done>
</task>

<task type="auto">
  <name>Task 4: Pass capture state from content script to Overlay</name>
  <files>entrypoints/content.tsx</files>
  <action>
Update content script to pass capture state to Overlay component:

1. Export the CaptureContext consumer hook at module level so Overlay can use it:

The CaptureProvider already creates a context. Update the render to expose capture state:

Actually, since Overlay is inside CaptureProvider, we need to use the context inside Overlay. Let's expose it properly.

2. In content.tsx, the CaptureProvider already wraps Overlay. Update Overlay to consume the context:

Move useCaptureState to a shared location. In content.tsx:
```typescript
// Export the context and hook for Overlay to use
export const CaptureContext = createContext<CaptureState | null>(null);

export function useCaptureState(): CaptureState | null {
  return useContext(CaptureContext);
}
```

3. In Overlay.tsx, import and use the context:
```typescript
// Import the capture state hook (need to get from content script context)
// Since Overlay runs in same context as content script, we can use window events instead

// Alternative: dispatch capture state via event
```

**Simpler approach:** Use custom events for capture state too (like transcript and llm-response):

In content.tsx, update CaptureProvider to dispatch events:
```typescript
function CaptureProvider({
  children,
  onCapture,
  getTranscriptSince,
  getRecentTranscript,
  getFullTranscript
}: CaptureProviderProps) {
  const captureState = useCaptureMode({
    onCapture,
    getTranscriptSince,
    getRecentTranscript,
    getFullTranscript,
  });

  // Dispatch capture state changes
  useEffect(() => {
    window.dispatchEvent(
      new CustomEvent<{ state: CaptureState }>('capture-state-update', {
        detail: { state: captureState },
      })
    );
  }, [captureState.isHolding, captureState.mode]);

  return (
    <CaptureContext.Provider value={captureState}>
      {children}
    </CaptureContext.Provider>
  );
}
```

In Overlay.tsx, add listener for capture state:
```typescript
const [captureState, setCaptureState] = useState<CaptureState | null>(null);

useEffect(() => {
  function handleCaptureUpdate(event: Event) {
    const customEvent = event as CustomEvent<{ state: CaptureState }>;
    setCaptureState(customEvent.detail.state);
  }

  window.addEventListener('capture-state-update', handleCaptureUpdate);
  return () => {
    window.removeEventListener('capture-state-update', handleCaptureUpdate);
  };
}, []);
```

Then pass captureState to CaptureIndicator (already done in Task 3 assuming prop is available).

Since Overlay manages its own captureState via events, remove the prop from OverlayProps interface.
  </action>
  <verify>
   - `npm run build` passes
   - Capture state updates flow to Overlay
   - CaptureIndicator shows during hold
  </verify>
  <done>Capture state flows from content script to Overlay via custom events</done>
</task>

<task type="checkpoint:human-verify" gate="blocking">
  <what-built>
    Complete LLM integration: hotkey capture triggers dual parallel LLM streaming, responses display in overlay in real-time.
  </what-built>
  <how-to-verify>
    1. Load the extension in Chrome
    2. Open Google Meet (you can use a test meeting at meet.google.com/new)
    3. Configure OpenRouter API key in extension popup settings
    4. Ensure a template is active (should have default templates)
    5. Start transcription (requires ElevenLabs key - or skip if not configured)
    6. Hold the capture hotkey (Ctrl+Shift+Space by default):
       - Should see red "Capturing question..." indicator at top of overlay
    7. Release the hotkey:
       - "Quick Hint" section should appear with fast model response
       - "Full Answer" section should stream comprehensive response
       - Footer should show "Streaming..." then "Ready"
    8. Alternatively, highlight some transcript text and press hotkey:
       - Should immediately trigger LLM request with highlighted text
       - Both responses should stream to overlay

    Expected behavior:
    - Fast hint appears within 2-3 seconds
    - Full answer streams in simultaneously
    - Both complete with no errors
    - Capture indicator disappears on release
  </how-to-verify>
  <resume-signal>Type "approved" if LLM flow works correctly, or describe issues</resume-signal>
</task>

</tasks>

<verification>
After all tasks complete:
1. `npm run build` passes without errors
2. LLM_STREAM/LLM_STATUS messages handled in content script
3. CaptureIndicator shows during hotkey hold
4. Real LLM responses display in ResponsePanel
5. End-to-end flow works: capture -> request -> streaming -> display
</verification>

<success_criteria>
- User holds hotkey and sees visual capture indicator
- User releases hotkey and fast hint appears within 2-3 seconds
- Full comprehensive answer streams simultaneously
- User can highlight text and press hotkey to send that specific text
- Both responses complete with clear indication
- No mock data used - real LLM responses only
</success_criteria>

<output>
After completion, create `.planning/phases/04-llm-integration/04-04-SUMMARY.md`
</output>
