---
phase: 04-llm-integration
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - package.json
  - src/services/llm/types.ts
  - src/services/llm/OpenRouterClient.ts
  - src/services/llm/PromptBuilder.ts
  - src/services/llm/index.ts
autonomous: true

must_haves:
  truths:
    - "OpenRouter streaming client can make SSE requests"
    - "Prompt builder substitutes $variables correctly"
    - "LLM types define request/response shapes"
  artifacts:
    - path: "src/services/llm/types.ts"
      provides: "OpenRouter request/response types"
      exports: ["StreamOptions", "DualLLMRequest", "StreamCallbacks"]
    - path: "src/services/llm/OpenRouterClient.ts"
      provides: "SSE streaming client for OpenRouter"
      exports: ["streamLLMResponse"]
    - path: "src/services/llm/PromptBuilder.ts"
      provides: "Prompt building with variable substitution"
      exports: ["buildPrompt"]
    - path: "src/services/llm/index.ts"
      provides: "Barrel export for LLM service"
  key_links:
    - from: "src/services/llm/PromptBuilder.ts"
      to: "src/utils/promptSubstitution.ts"
      via: "import substituteVariables"
      pattern: "substituteVariables"
    - from: "src/services/llm/OpenRouterClient.ts"
      to: "eventsource-parser"
      via: "npm dependency"
      pattern: "createParser"
---

<objective>
Install eventsource-parser and create the LLM service foundation: types, OpenRouter streaming client, and prompt builder.

Purpose: Establish the core LLM infrastructure that background.ts will use for dual parallel streaming requests.
Output: Complete `src/services/llm/` module with streaming capability and prompt construction.
</objective>

<execution_context>
@/Users/sasha-marchuk/.claude/get-shit-done/workflows/execute-plan.md
@/Users/sasha-marchuk/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/04-llm-integration/04-RESEARCH.md
@src/utils/promptSubstitution.ts
@src/store/types.ts
@src/types/transcript.ts
</context>

<tasks>

<task type="auto">
  <name>Task 1: Install eventsource-parser and create LLM types</name>
  <files>package.json, src/services/llm/types.ts</files>
  <action>
1. Install eventsource-parser:
   ```bash
   npm install eventsource-parser
   ```

2. Create `src/services/llm/types.ts` with:
   - `StreamOptions` interface: model, systemPrompt, userPrompt, maxTokens, onToken callback, onComplete callback, onError callback, abortSignal, apiKey
   - `DualLLMRequest` interface: question (captured text), recentContext (last N entries as string), fullTranscript (entire transcript as string), templateId
   - `StreamCallbacks` interface: onFastToken, onFullToken, onFastComplete, onFullComplete, onError (with model type 'fast' | 'full')
   - `OpenRouterChatMessage` interface: role ('system' | 'user' | 'assistant'), content (string)
   - `OpenRouterStreamChunk` interface: choices array with delta.content and finish_reason
   - Keep types minimal and focused on what OpenRouter API actually returns
  </action>
  <verify>
   - `npm ls eventsource-parser` shows package installed
   - `npx tsc --noEmit src/services/llm/types.ts` passes
  </verify>
  <done>eventsource-parser installed, LLM types defined with full streaming support</done>
</task>

<task type="auto">
  <name>Task 2: Create OpenRouter streaming client</name>
  <files>src/services/llm/OpenRouterClient.ts</files>
  <action>
Create `src/services/llm/OpenRouterClient.ts`:

1. Import createParser from eventsource-parser
2. Import StreamOptions from ./types

3. Implement `streamLLMResponse(options: StreamOptions): Promise<void>`:
   - Make fetch request to 'https://openrouter.ai/api/v1/chat/completions'
   - Headers: Authorization (Bearer token), Content-Type (application/json), HTTP-Referer (chrome.runtime.getURL('')), X-Title ('AI Interview Assistant')
   - Body: model, messages array (system + user), max_tokens, stream: true
   - Pass options.abortSignal to fetch for cancellation
   - Check response.ok, throw error with status and body text if not
   - Get ReadableStream reader from response.body
   - Create TextDecoder for UTF-8
   - Create parser with createParser:
     - onEvent handler: if data === '[DONE]' call onComplete and return
     - Parse JSON, extract choices[0].delta.content, call onToken if content exists
     - Check for error in parsed response, call onError if found
     - Check finish_reason === 'error', call onError if found
     - Wrap JSON.parse in try/catch, ignore parse errors (comment lines)
   - Read loop: while true, read from reader, break on done, feed decoder output to parser (use { stream: true })

4. Export the function
  </action>
  <verify>
   - `npx tsc --noEmit src/services/llm/OpenRouterClient.ts` passes
   - No import errors, all types resolve
  </verify>
  <done>OpenRouter SSE streaming client complete with proper error handling and cancellation support</done>
</task>

<task type="auto">
  <name>Task 3: Create prompt builder and barrel export</name>
  <files>src/services/llm/PromptBuilder.ts, src/services/llm/index.ts</files>
  <action>
1. Create `src/services/llm/PromptBuilder.ts`:

   - Import substituteVariables and PromptVariables from '../../utils/promptSubstitution'
   - Import PromptTemplate from '../../store/types'
   - Import DualLLMRequest from './types'

   - Define BuildPromptResult interface:
     - system: string (substituted system prompt)
     - user: string (substituted user prompt for fast hint - shorter context)
     - userFull: string (substituted user prompt for full answer - full context)

   - Implement `buildPrompt(request: DualLLMRequest, template: PromptTemplate): BuildPromptResult`:
     - Create PromptVariables with:
       - highlighted: request.question (the captured question)
       - recent: request.recentContext (last ~5 entries)
       - transcript: request.fullTranscript (everything)
     - Substitute variables in template.systemPrompt -> system
     - Substitute variables in template.userPromptTemplate -> user (for fast, emphasize brevity)
     - For userFull, append instruction for comprehensive answer to the user prompt
     - Return { system, user, userFull }

   - Export buildPrompt function

2. Create `src/services/llm/index.ts`:
   - Re-export all types from './types'
   - Re-export streamLLMResponse from './OpenRouterClient'
   - Re-export buildPrompt from './PromptBuilder'
  </action>
  <verify>
   - `npx tsc --noEmit src/services/llm/index.ts` passes
   - All exports resolve correctly
  </verify>
  <done>Prompt builder integrates with existing substitution utility, barrel export provides clean API</done>
</task>

</tasks>

<verification>
After all tasks complete:
1. `npm run build` passes without errors
2. `src/services/llm/` contains: types.ts, OpenRouterClient.ts, PromptBuilder.ts, index.ts
3. All imports chain correctly (index -> modules -> dependencies)
4. eventsource-parser appears in package.json dependencies
</verification>

<success_criteria>
- eventsource-parser installed and working
- LLM types fully defined for streaming use case
- OpenRouter client handles SSE parsing with eventsource-parser
- Prompt builder uses existing substituteVariables utility
- Clean barrel export at src/services/llm/index.ts
</success_criteria>

<output>
After completion, create `.planning/phases/04-llm-integration/04-01-SUMMARY.md`
</output>
