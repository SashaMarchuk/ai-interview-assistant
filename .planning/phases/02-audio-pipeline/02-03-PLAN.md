---
phase: 02-audio-pipeline
plan: 03
type: execute
wave: 2
depends_on: ["02-01"]
files_modified:
  - entrypoints/offscreen/main.ts
  - entrypoints/background.ts
autonomous: true

must_haves:
  truths:
    - "User's microphone can be captured as separate stream"
    - "Microphone audio converts to PCM via same AudioWorklet"
    - "MIC_AUDIO_CHUNK messages contain microphone PCM data"
  artifacts:
    - path: "entrypoints/offscreen/main.ts"
      provides: "Microphone capture function"
      contains: "startMicCapture"
  key_links:
    - from: "entrypoints/offscreen/main.ts"
      to: "Service Worker"
      via: "MIC_AUDIO_CHUNK message"
      pattern: "MIC_AUDIO_CHUNK"
---

<objective>
Implement microphone capture as a separate audio stream in the Offscreen Document.

Purpose: Capture user's voice (labeled "Me" in transcript) independently from tab audio - AUD-02 requirement.
Output: Microphone capture function in offscreen/main.ts, message handlers in background.ts.
</objective>

<execution_context>
@/Users/sasha-marchuk/.claude/get-shit-done/workflows/execute-plan.md
@/Users/sasha-marchuk/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/02-audio-pipeline/02-RESEARCH.md
@.planning/phases/02-audio-pipeline/02-01-SUMMARY.md

@src/types/messages.ts
@entrypoints/offscreen/main.ts
@entrypoints/background.ts
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add microphone capture to Offscreen Document</name>
  <files>entrypoints/offscreen/main.ts</files>
  <action>
Add microphone capture capability alongside tab capture:

1. Add module-level state for mic:
   - let micAudioContext: AudioContext | null = null
   - let micStream: MediaStream | null = null
   - let micWorkletNode: AudioWorkletNode | null = null

2. Create startMicCapture() async function:
   a. Request microphone via navigator.mediaDevices.getUserMedia:
      {
        audio: {
          echoCancellation: true,
          noiseSuppression: true,
          sampleRate: 16000
        }
      }
   b. Create separate AudioContext with { sampleRate: 16000 }
   c. Log actual sampleRate for debugging
   d. Create MediaStreamSource from micStream
   e. Do NOT connect to destination (we don't want to hear ourselves)
   f. Load AudioWorklet module (same pcm-processor.js)
   g. Create AudioWorkletNode
   h. Connect source to workletNode
   i. Set workletNode.port.onmessage to send MIC_AUDIO_CHUNK:
      chrome.runtime.sendMessage({
        type: 'MIC_AUDIO_CHUNK',
        chunk: event.data,
        timestamp: Date.now()
      })

3. Create stopMicCapture() function:
   a. Stop all tracks on micStream
   b. Disconnect micWorkletNode
   c. Close micAudioContext
   d. Set all to null

4. Add message type 'START_MIC_CAPTURE' handler:
   - Call startMicCapture()
   - Send response with success/error

5. Add message type 'STOP_MIC_CAPTURE' handler:
   - Call stopMicCapture()
   - Send response

6. Wrap in try/catch with CAPTURE_ERROR messages

NOTE: Microphone permission may need to be granted from an extension page first.
The Offscreen Document cannot show permission prompts. If getUserMedia fails with
NotAllowedError, log a helpful message about needing to grant permission via extension settings.
  </action>
  <verify>
TypeScript compiles:
```bash
cd /Users/sasha-marchuk/Work/Ai-Interview-Assistant-Chrome-Extension && npx tsc --noEmit
```
  </verify>
  <done>Microphone capture function exists, converts to PCM, sends MIC_AUDIO_CHUNK messages</done>
</task>

<task type="auto">
  <name>Task 2: Add mic message types and handlers to messages.ts and background.ts</name>
  <files>src/types/messages.ts, entrypoints/background.ts</files>
  <action>
1. In src/types/messages.ts:
   - Add 'START_MIC_CAPTURE' and 'STOP_MIC_CAPTURE' to MessageType union
   - Add interfaces:
     - StartMicCaptureMessage: { type: 'START_MIC_CAPTURE' }
     - StopMicCaptureMessage: { type: 'STOP_MIC_CAPTURE' }
   - Add to ExtensionMessage union

2. In entrypoints/background.ts:
   - Add handler for START_MIC_CAPTURE:
     - Forward to offscreen document
     - Return response
   - Add handler for STOP_MIC_CAPTURE:
     - Forward to offscreen document
     - Return response
   - Add handler for MIC_AUDIO_CHUNK:
     - Log chunk received (for verification)
     - Later phases will forward to transcription
     - Return { received: true }

Keep tab and mic capture independent - they can be started/stopped separately.
This allows future flexibility (e.g., user might want tab-only or mic-only capture).
  </action>
  <verify>
TypeScript compiles:
```bash
cd /Users/sasha-marchuk/Work/Ai-Interview-Assistant-Chrome-Extension && npx tsc --noEmit
```
Build succeeds:
```bash
cd /Users/sasha-marchuk/Work/Ai-Interview-Assistant-Chrome-Extension && npm run build
```
  </verify>
  <done>Mic message types defined, Service Worker handles MIC_AUDIO_CHUNK messages</done>
</task>

</tasks>

<verification>
1. TypeScript compiles: `npx tsc --noEmit`
2. Build succeeds: `npm run build`
3. startMicCapture and stopMicCapture functions exist in offscreen
4. MIC_AUDIO_CHUNK handler exists in background.ts
5. START_MIC_CAPTURE and STOP_MIC_CAPTURE message types defined
</verification>

<success_criteria>
- Microphone capture independent from tab capture
- PCM conversion uses same AudioWorklet processor
- MIC_AUDIO_CHUNK messages flow through message system
- No audio feedback (mic NOT connected to destination)
</success_criteria>

<output>
After completion, create `.planning/phases/02-audio-pipeline/02-03-SUMMARY.md`
</output>
