---
phase: 03-transcription
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - src/types/messages.ts
  - src/services/transcription/types.ts
  - src/services/transcription/ElevenLabsConnection.ts
  - src/services/transcription/AudioBuffer.ts
  - src/services/transcription/index.ts
autonomous: true
user_setup:
  - service: elevenlabs
    why: "Real-time speech-to-text transcription"
    env_vars:
      - name: ELEVENLABS_API_KEY
        source: "ElevenLabs Dashboard -> Profile -> API Keys"
    dashboard_config: []

must_haves:
  truths:
    - "Transcription message types exist for start/stop/partial/final/error"
    - "ElevenLabsConnection class can establish WebSocket to ElevenLabs API"
    - "AudioBuffer class can store chunks during disconnect and flush on reconnect"
  artifacts:
    - path: "src/types/messages.ts"
      provides: "Transcription message types"
      contains: "START_TRANSCRIPTION"
    - path: "src/services/transcription/ElevenLabsConnection.ts"
      provides: "WebSocket wrapper for ElevenLabs STT"
      exports: ["ElevenLabsConnection"]
    - path: "src/services/transcription/AudioBuffer.ts"
      provides: "Audio chunk buffering during disconnect"
      exports: ["AudioBuffer"]
    - path: "src/services/transcription/types.ts"
      provides: "Transcription-specific types"
      exports: ["TranscriptionConfig", "ServerMessage"]
  key_links:
    - from: "src/services/transcription/ElevenLabsConnection.ts"
      to: "wss://api.elevenlabs.io/v1/speech-to-text/realtime"
      via: "WebSocket constructor"
      pattern: "new WebSocket.*elevenlabs"
---

<objective>
Create transcription types and ElevenLabs WebSocket wrapper class.

Purpose: Establish the foundation for real-time speech-to-text by defining message types and creating a reusable WebSocket connection class that handles ElevenLabs Scribe v2 Realtime API protocol.

Output: Message types in src/types/messages.ts, transcription service module in src/services/transcription/ with connection wrapper and audio buffer classes.
</objective>

<execution_context>
@/Users/sasha-marchuk/.claude/get-shit-done/workflows/execute-plan.md
@/Users/sasha-marchuk/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/03-transcription/03-RESEARCH.md
@.planning/phases/03-transcription/03-CONTEXT.md
@src/types/messages.ts
@src/types/transcript.ts
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add transcription message types</name>
  <files>src/types/messages.ts</files>
  <action>
Extend the existing message types in src/types/messages.ts to add transcription lifecycle messages.

Add to MessageType union:
- 'START_TRANSCRIPTION' - Start transcription with API key
- 'STOP_TRANSCRIPTION' - Stop transcription
- 'TRANSCRIPTION_STARTED' - Confirm transcription is active
- 'TRANSCRIPTION_STOPPED' - Confirm transcription stopped
- 'TRANSCRIPTION_ERROR' - Error occurred (with source, error, canRetry)
- 'TRANSCRIPT_PARTIAL' - Interim text (source, text, timestamp)
- 'TRANSCRIPT_FINAL' - Finalized text (source, text, timestamp, id, speaker)
- 'TRANSCRIPT_UPDATE' - Full transcript array update

Add corresponding message interfaces:
- StartTranscriptionMessage: { type, apiKey }
- StopTranscriptionMessage: { type }
- TranscriptionStartedMessage: { type }
- TranscriptionStoppedMessage: { type }
- TranscriptionErrorMessage: { type, source: 'tab' | 'mic', error: string, canRetry: boolean }
- TranscriptPartialMessage: { type, source: 'tab' | 'mic', text: string, timestamp: number }
- TranscriptFinalMessage: { type, source: 'tab' | 'mic', text: string, timestamp: number, id: string, speaker: string }
- TranscriptUpdateMessage: { type, entries: TranscriptEntry[] }

Import TranscriptEntry from ./transcript.ts at top of file.
Add all new message interfaces to ExtensionMessage union type.

Follow existing code patterns - use BaseMessage extension, satisfies syntax in type guard if needed.
  </action>
  <verify>Run `npm run type-check` - no TypeScript errors</verify>
  <done>All transcription message types are defined and exported, ExtensionMessage union includes all new types</done>
</task>

<task type="auto">
  <name>Task 2: Create transcription service module</name>
  <files>
    src/services/transcription/types.ts
    src/services/transcription/ElevenLabsConnection.ts
    src/services/transcription/AudioBuffer.ts
    src/services/transcription/index.ts
  </files>
  <action>
Create src/services/transcription/ directory with 4 files:

**types.ts:**
- TranscriptionConfig interface: { apiKey, modelId?, languageCode?, source: 'tab' | 'mic' }
- ServerMessage type union from RESEARCH.md (session_started, partial_transcript, committed_transcript, committed_transcript_with_timestamps, error)
- Word interface: { text, start, end, type: 'word' | 'spacing', logprob? }
- ConnectionState type: 'disconnected' | 'connecting' | 'connected' | 'reconnecting'
- TranscriptCallback type: (text: string, isFinal: boolean, timestamp: number) => void

**AudioBuffer.ts:**
- Class AudioBuffer
- Constructor: maxChunks = 100 (default ~6 seconds of audio)
- Method add(chunk: ArrayBuffer): void - adds chunk, drops oldest if at max
- Method flush(): ArrayBuffer[] - returns all chunks and clears buffer
- Method clear(): void - clears buffer
- Property length: number (getter)

**ElevenLabsConnection.ts:**
- Class ElevenLabsConnection
- Constructor(config: TranscriptionConfig, onTranscript: TranscriptCallback, onError: (error: string, canRetry: boolean) => void)
- Private properties: ws, state, reconnectAttempts, audioBuffer, config, speakerLabel
- Method connect(): void - builds URL with query params (model_id=scribe_v2_realtime, audio_format=pcm_16000, commit_strategy=vad, include_timestamps=true, xi-api-key), creates WebSocket
- Method disconnect(): void - closes WebSocket, clears state
- Method sendAudio(chunk: ArrayBuffer): void - if connected, flush buffer + send; if disconnected, buffer chunk
- Private method handleOpen(): void - sets state=connected, resets reconnectAttempts
- Private method handleMessage(event: MessageEvent): void - parses ServerMessage, calls onTranscript for partial/committed
- Private method handleClose(): void - attempts reconnect with exponential backoff (max 3 attempts, 500ms base, 5000ms max with jitter)
- Private method handleError(error: Event): void - logs error, calls onError
- Private method reconnect(): void - implements backoff logic from RESEARCH.md
- Private method buildWebSocketUrl(): string - constructs wss://api.elevenlabs.io/v1/speech-to-text/realtime with query params
- Getter state: ConnectionState
- Getter speakerLabel: string (returns 'You' for mic, 'Interviewer' for tab)

Use btoa() for base64 encoding of PCM chunks (browser native, no library needed).
WebSocket sends JSON messages per ElevenLabs API: { message_type: 'input_audio_chunk', audio_base_64: string, commit: false, sample_rate: 16000 }

**index.ts:**
- Re-export ElevenLabsConnection from ./ElevenLabsConnection
- Re-export AudioBuffer from ./AudioBuffer
- Re-export types from ./types
  </action>
  <verify>Run `npm run type-check` - no TypeScript errors. Verify files exist: ls src/services/transcription/</verify>
  <done>Transcription service module exists with ElevenLabsConnection, AudioBuffer, and types. All exports are correct.</done>
</task>

</tasks>

<verification>
- [ ] `npm run type-check` passes with no errors
- [ ] src/types/messages.ts contains all transcription message types
- [ ] src/services/transcription/ directory exists with 4 files
- [ ] ElevenLabsConnection class exports correctly
- [ ] AudioBuffer class exports correctly
</verification>

<success_criteria>
1. All transcription message types are defined and type-safe
2. ElevenLabsConnection class implements WebSocket connection with reconnection logic
3. AudioBuffer class can store and flush audio chunks
4. No new npm dependencies needed (uses native WebSocket and btoa)
</success_criteria>

<output>
After completion, create `.planning/phases/03-transcription/03-01-SUMMARY.md`
</output>
