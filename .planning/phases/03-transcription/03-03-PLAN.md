---
phase: 03-transcription
plan: 03
type: execute
wave: 3
depends_on: ["03-02"]
files_modified:
  - entrypoints/content/index.tsx
  - src/overlay/TranscriptPanel.tsx
  - entrypoints/popup/App.tsx
autonomous: false

must_haves:
  truths:
    - "User sees live transcript in overlay when transcription is active"
    - "Interim results display with italic styling before finalization"
    - "Tab audio shows 'Interviewer' label, mic shows 'You' label"
    - "Transcript entries appear interlaced chronologically"
    - "User can start/stop transcription from popup"
  artifacts:
    - path: "entrypoints/content/index.tsx"
      provides: "Message listener for transcript updates"
      contains: "TRANSCRIPT_UPDATE"
    - path: "src/overlay/TranscriptPanel.tsx"
      provides: "Real transcript rendering (not mock)"
      contains: "entries"
    - path: "entrypoints/popup/App.tsx"
      provides: "Transcription start/stop controls"
      contains: "START_TRANSCRIPTION"
  key_links:
    - from: "entrypoints/content/index.tsx"
      to: "TranscriptPanel"
      via: "state prop"
      pattern: "entries.*TranscriptEntry"
    - from: "entrypoints/popup/App.tsx"
      to: "START_TRANSCRIPTION"
      via: "sendMessage"
      pattern: "sendMessage.*START_TRANSCRIPTION"
---

<objective>
Wire real transcript data to the overlay UI and add transcription controls.

Purpose: Replace mock transcript data with live transcription results. Enable users to start/stop transcription from the popup. Verify end-to-end transcription flow works.

Output: TranscriptPanel displays real data, Popup has transcription toggle, content script bridges transcript updates to overlay.
</objective>

<execution_context>
@/Users/sasha-marchuk/.claude/get-shit-done/workflows/execute-plan.md
@/Users/sasha-marchuk/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/03-transcription/03-RESEARCH.md
@.planning/phases/03-transcription/03-CONTEXT.md
@.planning/phases/03-transcription/03-02-SUMMARY.md
@entrypoints/content/index.tsx
@src/overlay/TranscriptPanel.tsx
@src/overlay/Overlay.tsx
@entrypoints/popup/App.tsx
@src/types/messages.ts
@src/types/transcript.ts
</context>

<tasks>

<task type="auto">
  <name>Task 1: Wire transcript updates to overlay</name>
  <files>
    entrypoints/content/index.tsx
    src/overlay/TranscriptPanel.tsx
    src/overlay/Overlay.tsx
  </files>
  <action>
**entrypoints/content/index.tsx:**

Add message listener to receive TRANSCRIPT_UPDATE from Service Worker:
1. Import TranscriptEntry type from src/types/transcript
2. Create module-level state: let currentTranscript: TranscriptEntry[] = []
3. Create a way to update the overlay - either:
   - Use a custom event to communicate with the React overlay
   - Or use a shared state solution

Recommended approach (custom event):
- Define CustomEvent type: TranscriptUpdateEvent extends CustomEvent<{ entries: TranscriptEntry[] }>
- In message listener, when receiving TRANSCRIPT_UPDATE:
  - Store entries in currentTranscript
  - Dispatch custom event: window.dispatchEvent(new CustomEvent('transcript-update', { detail: { entries } }))

**src/overlay/TranscriptPanel.tsx:**

Modify to accept real transcript data:
1. Add prop: entries: TranscriptEntry[] (already exists based on Phase 5)
2. Remove MOCK_TRANSCRIPT import and usage
3. Map entries with speaker-appropriate styling:
   - 'You' speaker: blue accent color (text-blue-400, border-blue-500)
   - 'Interviewer' speaker: purple accent color (text-purple-400, border-purple-500)
4. Show interim entries (isFinal: false) with:
   - Italic text (italic class)
   - 60% opacity (opacity-60 or text-white/60)
   - Append "..." to text
5. Format timestamp using Date.toLocaleTimeString() with hours/minutes format
6. Keep auto-scroll behavior via useAutoScroll hook

**src/overlay/Overlay.tsx:**

Wire transcript state to TranscriptPanel:
1. Add useState for transcript: const [transcript, setTranscript] = useState<TranscriptEntry[]>([])
2. Add useEffect to listen for 'transcript-update' custom event:
   - On event, call setTranscript(event.detail.entries)
   - Cleanup: remove event listener
3. Pass transcript to TranscriptPanel: <TranscriptPanel entries={transcript} />

Also add interim indicator state:
1. Track if any source has interim text (not shown in entries, but could show typing indicator)
2. For now, just ensure TranscriptPanel handles isFinal: false entries with distinct styling
  </action>
  <verify>Run `npm run type-check` - no errors. Run `npm run build` - succeeds. Manually verify overlay compiles.</verify>
  <done>Content script receives transcript updates, Overlay displays real transcript entries with correct speaker styling</done>
</task>

<task type="auto">
  <name>Task 2: Add transcription controls to Popup</name>
  <files>entrypoints/popup/App.tsx</files>
  <action>
Add transcription start/stop functionality to Popup.

**Add state:**
- const [isTranscribing, setIsTranscribing] = useState(false)
- const [transcriptionStatus, setTranscriptionStatus] = useState<string>('')

**Add handler functions:**

startTranscription():
- Get API key from store: const { apiKeys } = useStore()
- Check if apiKeys.elevenlabs exists, if not: setTranscriptionStatus('Set ElevenLabs API key in settings')
- Send START_TRANSCRIPTION message: chrome.runtime.sendMessage({ type: 'START_TRANSCRIPTION', apiKey: apiKeys.elevenlabs })
- On success: setIsTranscribing(true), setTranscriptionStatus('Transcribing...')
- On error: setTranscriptionStatus('Failed to start: ' + error)

stopTranscription():
- Send STOP_TRANSCRIPTION message: chrome.runtime.sendMessage({ type: 'STOP_TRANSCRIPTION' })
- setIsTranscribing(false)
- setTranscriptionStatus('Stopped')

**Add UI section after Audio Capture section:**

```tsx
<section className="mb-4">
  <h2 className="text-lg font-semibold mb-2">Transcription</h2>
  <div className="flex gap-2 mb-2">
    <button
      onClick={isTranscribing ? stopTranscription : startTranscription}
      disabled={!isCapturing} // Require audio capture first
      className={`px-4 py-2 rounded ${
        isTranscribing
          ? 'bg-red-600 hover:bg-red-700'
          : 'bg-green-600 hover:bg-green-700'
      } disabled:opacity-50 disabled:cursor-not-allowed`}
    >
      {isTranscribing ? 'Stop Transcription' : 'Start Transcription'}
    </button>
  </div>
  {transcriptionStatus && (
    <p className="text-sm text-gray-400">{transcriptionStatus}</p>
  )}
  {!isCapturing && (
    <p className="text-xs text-yellow-500">Start audio capture first</p>
  )}
</section>
```

**Import useStore** to get API key for transcription.
  </action>
  <verify>Run `npm run type-check` - no errors. Run `npm run build` - succeeds.</verify>
  <done>Popup has transcription start/stop button, validates API key exists, shows status</done>
</task>

<task type="checkpoint:human-verify" gate="blocking">
  <what-built>Complete transcription pipeline: audio capture -> ElevenLabs WebSocket -> transcript in overlay</what-built>
  <how-to-verify>
1. Load extension in Chrome (chrome://extensions, Load unpacked)
2. Open popup, go to Settings tab
3. Enter your ElevenLabs API key in the API Keys section
4. Navigate to a Google Meet call (or any tab for basic testing)
5. In popup, click "Start" to begin audio capture
6. Click "Start Transcription" button
7. Speak into your microphone - expect to see:
   - Your speech appearing as "You (HH:MM)" entries in overlay
   - Interim text in italic with "..." before finalizing
   - Text appearing within ~500ms of speaking
8. If testing with actual Meet call, interviewer speech should appear as "Interviewer (HH:MM)"
9. Click "Stop Transcription" to end
10. Verify entries remain visible after stopping
11. Check browser console for any errors

Expected behavior:
- Transcript text appears in real-time
- Speaker labels are correct (You vs Interviewer)
- Timestamps are formatted as time (HH:MM)
- Interim results show distinct styling then finalize
- No WebSocket errors in console
  </how-to-verify>
  <resume-signal>Type "approved" if transcription works, or describe any issues</resume-signal>
</task>

</tasks>

<verification>
- [ ] `npm run type-check` passes with no errors
- [ ] `npm run build` completes successfully
- [ ] Overlay displays transcript entries (not mock data)
- [ ] Speaker labels show "You" and "Interviewer"
- [ ] Interim text shows italic styling
- [ ] Popup has transcription controls
- [ ] Human verified end-to-end flow works
</verification>

<success_criteria>
1. User sees transcript text appearing within 500ms of spoken words (STT-01)
2. Partial (interim) text shows in a distinct style before finalizing (STT-02)
3. Tab audio transcript shows "Interviewer" label (STT-03 fallback - no diarization in realtime)
4. User's own speech appears labeled as "You" in transcript (STT-04)
5. Tab and microphone transcripts appear interlaced chronologically (STT-05)
</success_criteria>

<output>
After completion, create `.planning/phases/03-transcription/03-03-SUMMARY.md`
</output>
