---
phase: 07-integration
plan: 02
type: execute
wave: 1
depends_on: []
files_modified:
  - src/types/messages.ts
  - entrypoints/background.ts
  - entrypoints/content.tsx
  - entrypoints/offscreen/main.ts
autonomous: true

must_haves:
  truths:
    - "User sees 'Reconnecting...' indicator when STT WebSocket disconnects"
    - "User sees error indicator when LLM request fails"
    - "LLM automatically retries on failure (up to 3 times)"
    - "Overlay receives connection state updates via messages"
  artifacts:
    - path: "src/types/messages.ts"
      provides: "Connection state message types"
      contains: "CONNECTION_STATE"
    - path: "entrypoints/background.ts"
      provides: "LLM retry logic"
      contains: "retryCount|retry"
  key_links:
    - from: "entrypoints/offscreen/main.ts"
      to: "entrypoints/background.ts"
      via: "CONNECTION_STATE message"
      pattern: "CONNECTION_STATE"
    - from: "entrypoints/background.ts"
      to: "entrypoints/content.tsx"
      via: "sendLLMMessageToMeet broadcasts status"
      pattern: "sendLLMMessageToMeet"
---

<objective>
Add connection state broadcasting and LLM retry logic for graceful degradation.

Purpose: Per CONTEXT.md decisions, users should see visual feedback when services have issues (reconnecting, retrying). WebSocket disconnect should show reconnecting indicator. LLM failures should auto-retry with UI feedback.

Output: New message types for connection state, offscreen broadcasts connection changes, background has retry logic for LLM, content script receives and surfaces these states.
</objective>

<execution_context>
@/Users/sasha-marchuk/.claude/get-shit-done/workflows/execute-plan.md
@/Users/sasha-marchuk/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/07-integration/07-CONTEXT.md
@.planning/phases/07-integration/07-RESEARCH.md
@src/types/messages.ts
@entrypoints/background.ts
@entrypoints/content.tsx
@entrypoints/offscreen/main.ts
@src/services/transcription/ElevenLabsConnection.ts
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add connection state message types</name>
  <files>src/types/messages.ts</files>
  <action>
Add new message types for connection state broadcasting:

1. Add to MessageType union:
   ```typescript
   | 'CONNECTION_STATE'
   ```

2. Add new message interface:
   ```typescript
   // Connection state updates from offscreen to background to content
   export interface ConnectionStateMessage extends BaseMessage {
     type: 'CONNECTION_STATE';
     service: 'stt-tab' | 'stt-mic' | 'llm';
     state: 'connected' | 'disconnected' | 'reconnecting' | 'error';
     error?: string;
   }
   ```

3. Add to ExtensionMessage union:
   ```typescript
   | ConnectionStateMessage
   ```

4. Make sure TypeScript compilation passes (the switch statement in background.ts will need a case for this new type).
  </action>
  <verify>
Check type exists: `grep -q "CONNECTION_STATE" src/types/messages.ts`
Check interface: `grep -q "ConnectionStateMessage" src/types/messages.ts`
TypeScript check: `cd /Users/sasha-marchuk/Work/Ai-Interview-Assistant-Chrome-Extension && npx tsc --noEmit 2>&1 | head -20`
  </verify>
  <done>CONNECTION_STATE message type added to messages.ts</done>
</task>

<task type="auto">
  <name>Task 2: Broadcast connection state from offscreen and handle in background</name>
  <files>entrypoints/offscreen/main.ts, entrypoints/background.ts</files>
  <action>
1. Modify `entrypoints/offscreen/main.ts`:

   Add a helper function to broadcast connection state:
   ```typescript
   function broadcastConnectionState(
     service: 'stt-tab' | 'stt-mic',
     state: 'connected' | 'disconnected' | 'reconnecting' | 'error',
     error?: string
   ): void {
     chrome.runtime.sendMessage({
       type: 'CONNECTION_STATE',
       service,
       state,
       error,
     } satisfies ConnectionStateMessage).catch(() => {
       // Ignore - background might not be listening yet
     });
   }
   ```

   Update ElevenLabsConnection instantiation to include onConnect callback for connection state:
   - When WebSocket connects: broadcastConnectionState(source, 'connected')
   - When WebSocket disconnects: broadcastConnectionState(source, 'disconnected')
   - When reconnecting: The ElevenLabsConnection already has reconnecting state - need to expose this

   Note: ElevenLabsConnection.ts already has an onConnect callback option. Use it.
   For disconnect/reconnecting states, check if we can add a callback or poll the state.

   If ElevenLabsConnection doesn't expose reconnecting state change callbacks, modify the onError callback:
   - When error with canRetry=true, broadcast reconnecting state
   - When error with canRetry=false, broadcast error state

2. Modify `entrypoints/background.ts`:

   Add case for CONNECTION_STATE in handleMessage switch:
   ```typescript
   case 'CONNECTION_STATE': {
     // Forward connection state to content scripts for UI display
     const tabs = await chrome.tabs.query({ url: 'https://meet.google.com/*' });
     for (const tab of tabs) {
       if (tab.id) {
         chrome.tabs.sendMessage(tab.id, message).catch(() => {});
       }
     }
     return { received: true };
   }
   ```

   This forwards connection state from offscreen -> background -> content scripts.
  </action>
  <verify>
Check offscreen broadcasts: `grep -q "CONNECTION_STATE" entrypoints/offscreen/main.ts`
Check background handles: `grep -q "'CONNECTION_STATE'" entrypoints/background.ts`
Build passes: `cd /Users/sasha-marchuk/Work/Ai-Interview-Assistant-Chrome-Extension && npm run build`
  </verify>
  <done>Offscreen broadcasts connection state, background forwards to content scripts</done>
</task>

<task type="auto">
  <name>Task 3: Add LLM retry logic and receive connection state in content script</name>
  <files>entrypoints/background.ts, entrypoints/content.tsx</files>
  <action>
1. Modify `entrypoints/background.ts` - Add retry logic to handleLLMRequest:

   Per CONTEXT.md: "LLM request failure: Show error immediately, auto-retry, show retry indicator on UI"

   Add retry wrapper around streamLLMResponse calls:
   ```typescript
   const MAX_LLM_RETRIES = 3;
   const LLM_RETRY_DELAY_MS = 1000;

   async function streamWithRetry(
     params: StreamParams,
     model: 'fast' | 'full',
     responseId: string,
     retryCount = 0
   ): Promise<void> {
     try {
       await streamLLMResponse(params);
     } catch (error) {
       if (retryCount < MAX_LLM_RETRIES && !params.abortSignal?.aborted) {
         // Send retry status
         await sendLLMMessageToMeet({
           type: 'LLM_STATUS',
           responseId,
           model,
           status: 'error',
           error: `Retrying (${retryCount + 1}/${MAX_LLM_RETRIES})...`,
         });
         // Wait and retry
         await new Promise(r => setTimeout(r, LLM_RETRY_DELAY_MS * (retryCount + 1)));
         return streamWithRetry(params, model, responseId, retryCount + 1);
       }
       throw error; // Max retries exceeded, propagate error
     }
   }
   ```

   Update fastPromise and fullPromise to use streamWithRetry instead of direct streamLLMResponse.

   Also broadcast LLM connection state on error:
   - When LLM fails (after all retries): send CONNECTION_STATE with service='llm', state='error'

2. Modify `entrypoints/content.tsx`:

   Add handler for CONNECTION_STATE messages:
   ```typescript
   case 'CONNECTION_STATE': {
     // Dispatch custom event for Overlay to consume
     window.dispatchEvent(
       new CustomEvent<ConnectionStateEventDetail>('connection-state-update', {
         detail: {
           service: message.service,
           state: message.state,
           error: message.error,
         },
       })
     );
     return;
   }
   ```

   Add the event detail type near other event types:
   ```typescript
   export interface ConnectionStateEventDetail {
     service: 'stt-tab' | 'stt-mic' | 'llm';
     state: 'connected' | 'disconnected' | 'reconnecting' | 'error';
     error?: string;
   }
   ```

   This allows Overlay (from Plan 01) to listen for connection-state-update events and update HealthIndicator.
  </action>
  <verify>
Check retry logic: `grep -q "MAX_LLM_RETRIES\|streamWithRetry" entrypoints/background.ts`
Check content handles CONNECTION_STATE: `grep -q "CONNECTION_STATE" entrypoints/content.tsx`
Check event type exported: `grep -q "ConnectionStateEventDetail" entrypoints/content.tsx`
Build passes: `cd /Users/sasha-marchuk/Work/Ai-Interview-Assistant-Chrome-Extension && npm run build`
  </verify>
  <done>LLM has retry logic (3 retries), content script receives and dispatches connection state events</done>
</task>

</tasks>

<verification>
1. Build passes: `npm run build`
2. CONNECTION_STATE message type exists
3. Offscreen broadcasts STT connection state changes
4. Background forwards connection state to content scripts
5. Background retries LLM requests up to 3 times
6. Content script dispatches connection-state-update custom events
</verification>

<success_criteria>
1. New message type CONNECTION_STATE defined
2. Offscreen broadcasts when STT WebSocket connects/disconnects/reconnects
3. Background forwards connection state and has LLM retry logic
4. Content script receives connection state and dispatches to Overlay
5. Build succeeds with no TypeScript errors
</success_criteria>

<output>
After completion, create `.planning/phases/07-integration/07-02-SUMMARY.md`
</output>
