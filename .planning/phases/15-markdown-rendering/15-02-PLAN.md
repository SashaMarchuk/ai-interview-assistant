---
phase: 15-markdown-rendering
plan: 02
type: execute
wave: 2
depends_on: ["15-01"]
files_modified:
  - src/overlay/ResponsePanel.tsx
  - entrypoints/content.tsx
autonomous: true

must_haves:
  truths:
    - "LLM response text with headers, bold, italic, code blocks, and lists renders as formatted HTML in the overlay (not raw Markdown syntax)"
    - "Streaming responses render incrementally without visible flicker or full-content reparse lag"
    - "Token updates are batched via requestAnimationFrame to reduce render frequency during fast streaming"
    - "All Markdown styling works correctly inside the Shadow DOM overlay on a Google Meet page"
  artifacts:
    - path: "src/overlay/ResponsePanel.tsx"
      provides: "ResponsePanel using MemoizedMarkdown for formatted LLM response display"
      contains: "MemoizedMarkdown"
    - path: "entrypoints/content.tsx"
      provides: "Token batching via requestAnimationFrame in handleLLMStream"
      contains: "requestAnimationFrame"
  key_links:
    - from: "src/overlay/ResponsePanel.tsx"
      to: "src/components/markdown/MemoizedMarkdown.tsx"
      via: "import and render MemoizedMarkdown with response.fastHint and response.fullAnswer"
      pattern: "MemoizedMarkdown"
    - from: "entrypoints/content.tsx"
      to: "src/overlay/ResponsePanel.tsx"
      via: "llm-response-update custom event with batched token accumulation"
      pattern: "requestAnimationFrame"
---

<objective>
Wire the markdown rendering components into the existing ResponsePanel and add token batching to content.tsx for streaming performance. After this plan, LLM responses display as formatted Markdown with code highlighting, and streaming is smooth without O(n^2) reparse penalty.

Purpose: Complete the integration that makes markdown rendering visible to the user. Plan 01 built the components; this plan connects them to the live data flow.

Output: Modified ResponsePanel.tsx and content.tsx with markdown rendering and token batching active.
</objective>

<execution_context>
@/Users/sasha-marchuk/.claude/get-shit-done/workflows/execute-plan.md
@/Users/sasha-marchuk/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/15-markdown-rendering/15-RESEARCH.md
@.planning/phases/15-markdown-rendering/15-01-SUMMARY.md
@src/overlay/ResponsePanel.tsx
@entrypoints/content.tsx
@src/components/markdown/MemoizedMarkdown.tsx
</context>

<tasks>

<task type="auto">
  <name>Task 1: Integrate MemoizedMarkdown into ResponsePanel</name>
  <files>
    src/overlay/ResponsePanel.tsx
  </files>
  <action>
    Modify `ResponsePanel.tsx` to render LLM responses as formatted Markdown instead of plain text.

    1. Add import at top: `import { MemoizedMarkdown } from '../components/markdown/MemoizedMarkdown';`

    2. Replace the Fast Hint content section (currently line 83-84):
       - **Before:** `<div className="border-l-2 border-green-400/50 pl-2 text-white/90">{response.fastHint}</div>`
       - **After:** `<div className="border-l-2 border-green-400/50 pl-2"><MemoizedMarkdown content={response.fastHint} /></div>`
       - Remove `text-white/90` from the wrapper div — the markdown components apply their own text colors

    3. Replace the Full Answer content section (currently line 96-97):
       - **Before:** `<div className="border-l-2 border-purple-400/50 pl-2 text-white/90">{response.fullAnswer}</div>`
       - **After:** `<div className="border-l-2 border-purple-400/50 pl-2"><MemoizedMarkdown content={response.fullAnswer} /></div>`
       - Remove `text-white/90` from the wrapper div — same reason

    4. Keep everything else unchanged: StatusIndicator, error display, pending state, memo wrapper, section headers ("Quick Hint", "Full Answer"), conditional rendering logic.

    5. Do NOT modify the component's memo() wrapper or its props interface.
  </action>
  <verify>
    Run `npx tsc --noEmit` — no type errors.
    Verify the import is present: `grep MemoizedMarkdown src/overlay/ResponsePanel.tsx`
    Verify plain text rendering is replaced: `grep -c 'response.fastHint}' src/overlay/ResponsePanel.tsx` should return 0 (no bare interpolation)
  </verify>
  <done>
    ResponsePanel renders both fastHint and fullAnswer through MemoizedMarkdown. Raw markdown syntax (**, ##, ```) is no longer visible to users — it renders as formatted HTML with proper styling.
  </done>
</task>

<task type="auto">
  <name>Task 2: Add token batching to content.tsx for streaming performance</name>
  <files>
    entrypoints/content.tsx
  </files>
  <action>
    Add requestAnimationFrame-based token batching to `handleLLMStream` in `content.tsx` to reduce render frequency during fast streaming.

    1. Add module-level batching state variables after the existing `activeResponseId` declaration (around line 51):
       ```typescript
       // Token batching for streaming performance
       // Accumulates tokens and flushes on animation frame to reduce React re-renders
       let pendingFastTokens = '';
       let pendingFullTokens = '';
       let tokenBatchRafId: number | null = null;
       ```

    2. Create a `flushPendingTokens` function that:
       - Checks if there are any pending tokens (fast or full)
       - If yes, builds a response update from `currentLLMResponse` with accumulated tokens appended
       - Dispatches the update via `dispatchLLMResponseUpdate`
       - Resets `pendingFastTokens` and `pendingFullTokens` to empty strings
       - Sets `tokenBatchRafId` to null
       ```typescript
       function flushPendingTokens(): void {
         tokenBatchRafId = null;
         if (!pendingFastTokens && !pendingFullTokens) return;
         if (!currentLLMResponse) return;

         const response = { ...currentLLMResponse };
         if (pendingFastTokens) {
           response.fastHint = (response.fastHint || '') + pendingFastTokens;
           pendingFastTokens = '';
         }
         if (pendingFullTokens) {
           response.fullAnswer = (response.fullAnswer || '') + pendingFullTokens;
           pendingFullTokens = '';
         }
         response.status = 'streaming';
         dispatchLLMResponseUpdate(response);
       }
       ```

    3. Modify `handleLLMStream` (currently line 94-107) to batch tokens instead of dispatching immediately:
       ```typescript
       function handleLLMStream(message: LLMStreamMessage): void {
         // Ensure response exists (initializes if needed, returns null if stale)
         const response = ensureLLMResponse(message.responseId);
         if (!response) return;

         // Accumulate tokens into pending buffers
         if (message.model === 'fast') {
           pendingFastTokens += message.token;
         } else if (message.model === 'full') {
           pendingFullTokens += message.token;
         }

         // Schedule a flush on the next animation frame (if not already scheduled)
         if (tokenBatchRafId === null) {
           tokenBatchRafId = requestAnimationFrame(flushPendingTokens);
         }
       }
       ```

    4. In `handleLLMStatus` function: when status transitions to 'complete' or 'error', flush any pending tokens FIRST before dispatching the status change. Add at the beginning of `handleLLMStatus`:
       ```typescript
       // Flush any pending tokens before status transition
       if (tokenBatchRafId !== null) {
         cancelAnimationFrame(tokenBatchRafId);
         flushPendingTokens();
       }
       ```
       This ensures no tokens are lost when the stream completes.

    5. In `initLLMResponse` function: reset batching state when a new request starts. Add after `activeResponseId = responseId;`:
       ```typescript
       // Reset token batching state
       if (tokenBatchRafId !== null) {
         cancelAnimationFrame(tokenBatchRafId);
         tokenBatchRafId = null;
       }
       pendingFastTokens = '';
       pendingFullTokens = '';
       ```
  </action>
  <verify>
    Run `npx tsc --noEmit` — no type errors.
    Run `npm run dev` — build succeeds.
    Verify token batching is present: `grep requestAnimationFrame entrypoints/content.tsx`
    Verify flush on complete: `grep flushPendingTokens entrypoints/content.tsx`
  </verify>
  <done>
    Token batching via requestAnimationFrame is active in handleLLMStream. Tokens accumulate in pending buffers and flush once per animation frame (~16ms batches), reducing React re-renders during fast streaming. Pending tokens are flushed before status transitions (complete/error) to prevent data loss.
  </done>
</task>

</tasks>

<verification>
1. `npx tsc --noEmit` passes with zero errors
2. `npm run dev` builds successfully
3. ResponsePanel.tsx imports and uses MemoizedMarkdown for both fastHint and fullAnswer
4. content.tsx uses requestAnimationFrame for token batching in handleLLMStream
5. Token batching flushes on status transitions (no token loss at stream end)
6. No changes to component interfaces, message types, or other files
</verification>

<success_criteria>
- LLM responses in the overlay render as formatted Markdown (headers, bold, italic, lists, code blocks with highlighting)
- Streaming tokens are batched via requestAnimationFrame (~16ms windows) reducing render frequency
- Pending tokens flush before status transitions to prevent data loss
- All styling works inside Shadow DOM (no style leakage, correct colors)
- TypeScript compiles cleanly, build succeeds
</success_criteria>

<output>
After completion, create `.planning/phases/15-markdown-rendering/15-02-SUMMARY.md`
</output>
