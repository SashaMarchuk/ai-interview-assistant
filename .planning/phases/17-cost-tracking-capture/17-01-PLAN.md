---
phase: 17-cost-tracking-capture
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - src/services/llm/types.ts
  - src/services/llm/providers/LLMProvider.ts
  - src/services/llm/providers/streamSSE.ts
  - src/services/llm/providers/OpenAIProvider.ts
  - src/services/llm/pricing.ts
  - src/services/llm/index.ts
  - src/types/messages.ts
  - src/types/transcript.ts
  - entrypoints/background.ts
autonomous: true

must_haves:
  truths:
    - "Token usage (prompt, completion, reasoning tokens) is extracted from the final SSE chunk of every streaming LLM response"
    - "Token usage is extracted from non-streaming JSON fallback responses (reasoning models)"
    - "OpenAI requests include stream_options: { include_usage: true } to opt-in to usage reporting"
    - "Cost is calculated client-side for OpenAI using static pricing table, and from usage.cost for OpenRouter"
    - "Background service worker broadcasts LLM_COST messages to content scripts after each model completes"
  artifacts:
    - path: "src/services/llm/pricing.ts"
      provides: "Static OpenAI pricing table and calculateCost function"
      exports: ["OPENAI_PRICING", "calculateCost"]
    - path: "src/services/llm/types.ts"
      provides: "TokenUsage interface"
      contains: "TokenUsage"
    - path: "src/types/messages.ts"
      provides: "LLM_COST message type and LLMCostMessage interface"
      contains: "LLM_COST"
    - path: "src/types/transcript.ts"
      provides: "Cost fields on LLMResponse interface"
      contains: "costUSD"
  key_links:
    - from: "src/services/llm/providers/streamSSE.ts"
      to: "ProviderStreamOptions.onUsage"
      via: "onUsage callback invoked with usage object from final SSE chunk"
      pattern: "options\\.onUsage"
    - from: "entrypoints/background.ts"
      to: "src/services/llm/pricing.ts"
      via: "calculateCost called in onUsage callback"
      pattern: "calculateCost"
    - from: "entrypoints/background.ts"
      to: "broadcastToMeetTabs"
      via: "LLM_COST message sent after cost calculation"
      pattern: "LLM_COST"
---

<objective>
Implement the complete cost tracking data pipeline: types, pricing table, usage extraction from streaming responses, cost calculation, and LLM_COST message broadcasting from background to content scripts.

Purpose: Establish the full backend data flow so that token usage is captured from every LLM request and cost data is available for the UI to display.
Output: TokenUsage types, pricing module, extended streaming pipeline with onUsage callback, LLM_COST message type, and background handler that calculates and broadcasts cost data.
</objective>

<execution_context>
@/Users/sasha-marchuk/.claude/get-shit-done/workflows/execute-plan.md
@/Users/sasha-marchuk/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/17-cost-tracking-capture/17-RESEARCH.md

@src/services/llm/types.ts
@src/services/llm/providers/LLMProvider.ts
@src/services/llm/providers/streamSSE.ts
@src/services/llm/providers/OpenAIProvider.ts
@src/services/llm/providers/OpenRouterProvider.ts
@src/services/llm/index.ts
@src/types/messages.ts
@src/types/transcript.ts
@entrypoints/background.ts
</context>

<tasks>

<task type="auto">
  <name>Task 1: Types, pricing module, and streaming pipeline extension</name>
  <files>
    src/services/llm/types.ts
    src/services/llm/providers/LLMProvider.ts
    src/services/llm/providers/streamSSE.ts
    src/services/llm/providers/OpenAIProvider.ts
    src/services/llm/pricing.ts
    src/services/llm/index.ts
    src/types/messages.ts
    src/types/transcript.ts
  </files>
  <action>
    **1. Add TokenUsage interface to `src/services/llm/types.ts`:**
    ```typescript
    export interface TokenUsage {
      promptTokens: number;
      completionTokens: number;
      reasoningTokens: number;
      totalTokens: number;
      /** OpenRouter returns cost directly; undefined for OpenAI */
      providerCost?: number;
    }
    ```

    **2. Add `onUsage` callback to `ProviderStreamOptions` in `src/services/llm/providers/LLMProvider.ts`:**
    Add an optional `onUsage` field:
    ```typescript
    /** Optional callback for token usage data from final chunk */
    onUsage?: (usage: TokenUsage) => void;
    ```
    Import `TokenUsage` from `../types`.

    **3. Extend `StreamChunk` in `src/services/llm/providers/streamSSE.ts`:**
    Add optional `usage` field to the `StreamChunk` interface:
    ```typescript
    usage?: {
      prompt_tokens: number;
      completion_tokens: number;
      total_tokens: number;
      prompt_tokens_details?: { cached_tokens?: number; cache_write_tokens?: number };
      completion_tokens_details?: { reasoning_tokens?: number };
      cost?: number; // OpenRouter only
    };
    ```
    In the `onEvent` handler, after parsing the JSON chunk, add usage extraction BEFORE the `choices` processing:
    ```typescript
    if (chunk.usage && options.onUsage) {
      options.onUsage({
        promptTokens: chunk.usage.prompt_tokens,
        completionTokens: chunk.usage.completion_tokens,
        reasoningTokens: chunk.usage.completion_tokens_details?.reasoning_tokens ?? 0,
        totalTokens: chunk.usage.total_tokens,
        providerCost: chunk.usage.cost,
      });
    }
    ```
    Import `TokenUsage` from `../types`.
    ALSO in the non-streaming JSON fallback path (the `application/json` branch), extract usage from `json.usage` and call `options.onUsage` if present. The JSON response type needs to be extended to include `usage`.

    **4. Add `stream_options` to OpenAI request body in `src/services/llm/providers/OpenAIProvider.ts`:**
    In `streamResponse()`, add `stream_options: { include_usage: true }` to the `body` object. This is required for OpenAI to include usage in streaming responses.

    **5. Create `src/services/llm/pricing.ts` (NEW FILE):**
    ```typescript
    /**
     * Static pricing table for OpenAI models and cost calculation.
     * OpenRouter returns cost directly in the API response; this table
     * is only used for OpenAI provider where cost must be calculated.
     *
     * Prices as of Feb 2026 from https://openai.com/api/pricing/
     */

    interface ModelPricing {
      inputPerMillion: number;  // USD per 1M input tokens
      outputPerMillion: number; // USD per 1M output tokens
    }

    const OPENAI_PRICING: Record<string, ModelPricing> = {
      'gpt-4o':        { inputPerMillion: 2.50,  outputPerMillion: 10.00 },
      'gpt-4o-mini':   { inputPerMillion: 0.15,  outputPerMillion: 0.60 },
      'gpt-4.1':       { inputPerMillion: 2.00,  outputPerMillion: 8.00 },
      'gpt-4.1-mini':  { inputPerMillion: 0.40,  outputPerMillion: 1.60 },
      'gpt-4.1-nano':  { inputPerMillion: 0.10,  outputPerMillion: 0.40 },
      'gpt-5':         { inputPerMillion: 1.25,  outputPerMillion: 10.00 },
      'gpt-5-mini':    { inputPerMillion: 0.25,  outputPerMillion: 2.00 },
      'gpt-5-nano':    { inputPerMillion: 0.05,  outputPerMillion: 0.40 },
      'o1':            { inputPerMillion: 15.00, outputPerMillion: 60.00 },
      'o1-mini':       { inputPerMillion: 1.10,  outputPerMillion: 4.40 },
      'o3-mini':       { inputPerMillion: 1.10,  outputPerMillion: 4.40 },
      'o4-mini':       { inputPerMillion: 1.10,  outputPerMillion: 4.40 },
    };

    /**
     * Calculate cost in USD from token counts and model ID.
     * For OpenRouter, use providerCost directly (more accurate).
     * For OpenAI, calculate from static pricing table.
     * Returns 0 for unknown models.
     */
    export function calculateCost(
      modelId: string,
      promptTokens: number,
      completionTokens: number,
      providerCost?: number,
    ): number {
      // If provider returned cost directly (OpenRouter), use it
      if (providerCost != null && providerCost > 0) {
        return providerCost;
      }
      // Calculate from static table (OpenAI)
      const pricing = OPENAI_PRICING[modelId];
      if (!pricing) return 0;
      const inputCost = (promptTokens / 1_000_000) * pricing.inputPerMillion;
      const outputCost = (completionTokens / 1_000_000) * pricing.outputPerMillion;
      return inputCost + outputCost;
    }

    export { OPENAI_PRICING };
    ```

    **6. Add `LLM_COST` message type to `src/types/messages.ts`:**
    - Add `'LLM_COST'` to the `MessageType` union
    - Add `LLMCostMessage` interface:
      ```typescript
      export interface LLMCostMessage extends BaseMessage {
        type: 'LLM_COST';
        responseId: string;
        model: LLMModelType; // 'fast' | 'full'
        promptTokens: number;
        completionTokens: number;
        reasoningTokens: number;
        totalTokens: number;
        costUSD: number;
      }
      ```
    - Add `LLMCostMessage` to the `ExtensionMessage` union

    **7. Add cost fields to `LLMResponse` in `src/types/transcript.ts`:**
    Add to `LLMResponse` interface:
    ```typescript
    /** Cost of fast model response in USD */
    fastCostUSD?: number;
    /** Cost of full model response in USD */
    fullCostUSD?: number;
    /** Total combined cost in USD (fast + full) */
    totalCostUSD?: number;
    ```

    **8. Re-export pricing and TokenUsage from `src/services/llm/index.ts`:**
    Add:
    ```typescript
    export { calculateCost, OPENAI_PRICING } from './pricing';
    export type { TokenUsage } from './types';
    ```
  </action>
  <verify>
    Run `npx tsc --noEmit` — should pass with zero errors.
    Run `npx eslint src/services/llm/ src/types/` — no new lint errors.
  </verify>
  <done>
    TokenUsage interface defined, StreamChunk extended with usage field, onUsage callback on ProviderStreamOptions, streamSSE extracts usage from both SSE and JSON paths, OpenAI adds stream_options, pricing.ts exists with calculateCost, LLM_COST message type defined, LLMResponse has cost fields, all re-exported from index.ts. TypeScript compiles cleanly.
  </done>
</task>

<task type="auto">
  <name>Task 2: Background service worker cost calculation and LLM_COST broadcasting</name>
  <files>
    entrypoints/background.ts
  </files>
  <action>
    **1. Import new types and utilities at top of `entrypoints/background.ts`:**
    Add imports for `calculateCost`, `TokenUsage`, and `LLMCostMessage`:
    ```typescript
    import { calculateCost, type TokenUsage } from '../src/services/llm';
    import type { LLMCostMessage } from '../src/types/messages';
    ```

    **2. Add `onUsage` callback to `StreamWithRetryParams`:**
    Add `onUsage?: (usage: TokenUsage) => void` to the `StreamWithRetryParams` interface.

    **3. Pass `onUsage` through `streamWithRetry` to `provider.streamResponse`:**
    In the `streamWithRetry` function, pass `params.onUsage` as `onUsage` in the options object to `params.provider.streamResponse()`.

    **4. Add `onUsage` callback in `fireModelRequest`:**
    In the `fireModelRequest` function, add an `onUsage` callback to the `streamWithRetry` call. This callback should:
    - Receive the `TokenUsage` from the streaming pipeline
    - Call `calculateCost()` to get USD cost, passing `config.modelId` as model, and `usage.providerCost` for OpenRouter
    - Broadcast an `LLM_COST` message to content scripts:
      ```typescript
      onUsage: (usage: TokenUsage) => {
        const costUSD = calculateCost(
          config.modelId,
          usage.promptTokens,
          usage.completionTokens,
          usage.providerCost,
        );
        sendLLMMessageToMeet({
          type: 'LLM_COST',
          responseId,
          model: config.modelType,
          promptTokens: usage.promptTokens,
          completionTokens: usage.completionTokens,
          reasoningTokens: usage.reasoningTokens,
          totalTokens: usage.totalTokens,
          costUSD,
        } as LLMCostMessage);
      },
      ```

    **5. Update `sendLLMMessageToMeet` signature to accept `LLMCostMessage`:**
    Change the type parameter to also accept `LLMCostMessage`:
    ```typescript
    async function sendLLMMessageToMeet(
      message: LLMStreamMessage | LLMStatusMessage | LLMCostMessage,
    ): Promise<void> {
    ```

    **6. Handle `LLM_COST` in the background message handler switch:**
    Add a case for `'LLM_COST'` in the `handleMessage` switch that returns `{ received: true }` (just like LLM_STREAM and LLM_STATUS -- background doesn't need to process its own cost messages, they're only consumed by content scripts).

    **Note on exhaustive check:** The `default` case in `handleMessage` uses `never` exhaustive check. Adding `'LLM_COST'` to `MessageType` and handling it in the switch is required to prevent TypeScript compile errors.
  </action>
  <verify>
    Run `npx tsc --noEmit` — should pass with zero errors.
    Run `npx eslint entrypoints/background.ts` — no new lint errors.
  </verify>
  <done>
    Background service worker calculates cost from token usage via calculateCost(), broadcasts LLM_COST messages to content scripts after each model completes, handles LLM_COST in its own message switch for exhaustive type checking. Full data pipeline from API response -> usage extraction -> cost calculation -> message broadcast is complete.
  </done>
</task>

</tasks>

<verification>
1. `npx tsc --noEmit` passes — all new types, interfaces, and message types compile cleanly
2. `npx eslint .` passes — no lint errors in any modified file
3. Pricing table contains all models from OPENAI_MODELS list
4. streamSSE extracts usage from both SSE final chunk (empty choices) and non-streaming JSON fallback
5. OpenAI provider sends `stream_options: { include_usage: true }` in request body
6. Background onUsage callback calculates cost and broadcasts LLM_COST message
7. LLM_COST message contains responseId, model type, token breakdown, and costUSD
</verification>

<success_criteria>
- TypeScript compiles with zero errors
- All 12 model IDs in OPENAI_MODELS have corresponding entries in OPENAI_PRICING
- onUsage callback is wired through: streamSSE -> ProviderStreamOptions -> provider.streamResponse -> streamWithRetry -> fireModelRequest -> calculateCost -> broadcastToMeetTabs
- LLM_COST is a valid MessageType and handled in background.ts switch
</success_criteria>

<output>
After completion, create `.planning/phases/17-cost-tracking-capture/17-01-SUMMARY.md`
</output>
