---
phase: 08-openai-provider-support
plan: 03
type: execute
wave: 2
depends_on: ["08-01", "08-02"]
files_modified:
  - entrypoints/background.ts
  - src/services/llm/OpenRouterClient.ts
autonomous: false

user_setup:
  - service: openai
    why: "OpenAI API access for direct model calls"
    env_vars:
      - name: OpenAI API Key
        source: "platform.openai.com -> API keys -> Create new secret key"

must_haves:
  truths:
    - "LLM requests use provider abstraction instead of direct OpenRouter calls"
    - "Provider is automatically selected based on model ID"
    - "OpenAI models work when OpenAI key is configured"
    - "OpenRouter models work when OpenRouter key is configured"
    - "Templates with unavailable models show appropriate error"
  artifacts:
    - path: "entrypoints/background.ts"
      provides: "Provider-aware LLM request handling"
      contains: "resolveProviderForModel"
    - path: "src/services/llm/OpenRouterClient.ts"
      provides: "Deprecated notice, delegates to provider"
      contains: "deprecated"
  key_links:
    - from: "entrypoints/background.ts"
      to: "src/services/llm/providers"
      via: "resolveProviderForModel import"
      pattern: "import.*resolveProviderForModel.*providers"
    - from: "entrypoints/background.ts"
      to: "LLMProvider.streamResponse"
      via: "provider.streamResponse call"
      pattern: "provider\\.streamResponse"
---

<objective>
Integrate the provider abstraction into background.ts and verify end-to-end functionality with both OpenAI and OpenRouter.

Purpose: Wire up the provider layer created in Plan 01 with the store updates from Plan 02, replacing direct OpenRouter calls with provider-agnostic code that automatically selects the right provider based on model ID.

Output: Working multi-provider LLM integration that seamlessly uses OpenAI or OpenRouter based on configuration
</objective>

<execution_context>
@/Users/sasha-marchuk/.claude/get-shit-done/workflows/execute-plan.md
@/Users/sasha-marchuk/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/08-openai-provider-support/08-RESEARCH.md

# Prior plan summaries (dependency context)
@.planning/phases/08-openai-provider-support/08-01-SUMMARY.md
@.planning/phases/08-openai-provider-support/08-02-SUMMARY.md

# Files to modify
@entrypoints/background.ts
@src/services/llm/index.ts
</context>

<tasks>

<task type="auto">
  <name>Task 1: Update background.ts to use provider abstraction</name>
  <files>entrypoints/background.ts</files>
  <action>
Modify `entrypoints/background.ts` to use the provider abstraction:

1. Update imports at the top of the file:

Change from:
```typescript
import { streamLLMResponse, buildPrompt } from '../src/services/llm';
```
To:
```typescript
import { buildPrompt, resolveProviderForModel, type LLMProvider } from '../src/services/llm';
```

2. Update the `streamWithRetry` function:

Change the function signature and implementation to use provider instead of streamLLMResponse:

```typescript
interface StreamWithRetryParams {
  provider: LLMProvider;
  model: string;
  systemPrompt: string;
  userPrompt: string;
  maxTokens: number;
  apiKey: string;
  onToken: (token: string) => void;
  onComplete: () => void;
  onError: (error: Error) => void;
  abortSignal?: AbortSignal;
}

async function streamWithRetry(
  params: StreamWithRetryParams,
  modelType: 'fast' | 'full',
  responseId: string,
  retryCount = 0
): Promise<void> {
  try {
    await params.provider.streamResponse({
      model: params.model,
      systemPrompt: params.systemPrompt,
      userPrompt: params.userPrompt,
      maxTokens: params.maxTokens,
      apiKey: params.apiKey,
      onToken: params.onToken,
      onComplete: params.onComplete,
      onError: params.onError,
      abortSignal: params.abortSignal,
    });
  } catch (error) {
    // ... existing retry logic unchanged ...
  }
}
```

3. Update `handleLLMRequest` function:

Replace the API key validation and streaming calls with provider resolution:

```typescript
async function handleLLMRequest(
  responseId: string,
  question: string,
  recentContext: string,
  fullTranscript: string,
  templateId: string
): Promise<void> {
  // Get store state for settings and templates
  const state = useStore.getState();
  const { apiKeys, models, templates } = state;

  // Find template
  const template = templates.find((t) => t.id === templateId);
  if (!template) {
    await sendLLMMessageToMeet({
      type: 'LLM_STATUS',
      responseId,
      model: 'both',
      status: 'error',
      error: `Template not found: ${templateId}`,
    });
    return;
  }

  // Resolve provider for fast model
  const fastResolution = resolveProviderForModel(models.fastModel, {
    openAI: apiKeys.openAI,
    openRouter: apiKeys.openRouter,
  });

  // Resolve provider for full model
  const fullResolution = resolveProviderForModel(models.fullModel, {
    openAI: apiKeys.openAI,
    openRouter: apiKeys.openRouter,
  });

  // Check if either model couldn't be resolved
  if (!fastResolution && !fullResolution) {
    await sendLLMMessageToMeet({
      type: 'LLM_STATUS',
      responseId,
      model: 'both',
      status: 'error',
      error: 'No LLM provider configured. Add an OpenAI or OpenRouter API key in settings.',
    });
    return;
  }

  // Build prompts using the template
  const prompts = buildPrompt(
    { question, recentContext, fullTranscript, templateId },
    template
  );

  // Create abort controller for cancellation
  const abortController = new AbortController();
  activeAbortControllers.set(responseId, abortController);

  // Start keep-alive to prevent service worker termination
  startKeepAlive();

  // Track completion state
  let fastComplete = false;
  let fullComplete = false;

  const checkAllComplete = () => {
    if (fastComplete && fullComplete) {
      activeAbortControllers.delete(responseId);
      if (activeAbortControllers.size === 0) {
        stopKeepAlive();
      }
    }
  };

  // Send initial pending status
  await sendLLMMessageToMeet({
    type: 'LLM_STATUS',
    responseId,
    model: 'both',
    status: 'pending',
  });

  // Fire fast model request (if provider available)
  let fastPromise: Promise<void> = Promise.resolve();
  if (fastResolution) {
    fastPromise = streamWithRetry(
      {
        provider: fastResolution.provider,
        model: fastResolution.model,
        systemPrompt: prompts.system,
        userPrompt: prompts.user,
        maxTokens: 300,
        apiKey: fastResolution.provider.id === 'openai' ? apiKeys.openAI : apiKeys.openRouter,
        onToken: (token) => {
          sendLLMMessageToMeet({
            type: 'LLM_STREAM',
            responseId,
            model: 'fast',
            token,
          });
        },
        onComplete: () => {
          fastComplete = true;
          sendLLMMessageToMeet({
            type: 'LLM_STATUS',
            responseId,
            model: 'fast',
            status: 'complete',
          });
          checkAllComplete();
        },
        onError: (error) => {
          fastComplete = true;
          sendLLMMessageToMeet({
            type: 'LLM_STATUS',
            responseId,
            model: 'fast',
            status: 'error',
            error: error.message,
          });
          checkAllComplete();
        },
        abortSignal: abortController.signal,
      },
      'fast',
      responseId
    );
  } else {
    fastComplete = true;
    await sendLLMMessageToMeet({
      type: 'LLM_STATUS',
      responseId,
      model: 'fast',
      status: 'error',
      error: `Model ${models.fastModel} not available with current API keys`,
    });
  }

  // Fire full model request (if provider available)
  let fullPromise: Promise<void> = Promise.resolve();
  if (fullResolution) {
    fullPromise = streamWithRetry(
      {
        provider: fullResolution.provider,
        model: fullResolution.model,
        systemPrompt: prompts.system,
        userPrompt: prompts.userFull,
        maxTokens: 2000,
        apiKey: fullResolution.provider.id === 'openai' ? apiKeys.openAI : apiKeys.openRouter,
        onToken: (token) => {
          sendLLMMessageToMeet({
            type: 'LLM_STREAM',
            responseId,
            model: 'full',
            token,
          });
        },
        onComplete: () => {
          fullComplete = true;
          sendLLMMessageToMeet({
            type: 'LLM_STATUS',
            responseId,
            model: 'full',
            status: 'complete',
          });
          checkAllComplete();
        },
        onError: (error) => {
          fullComplete = true;
          sendLLMMessageToMeet({
            type: 'LLM_STATUS',
            responseId,
            model: 'full',
            status: 'error',
            error: error.message,
          });
          checkAllComplete();
        },
        abortSignal: abortController.signal,
      },
      'full',
      responseId
    );
  } else {
    fullComplete = true;
    await sendLLMMessageToMeet({
      type: 'LLM_STATUS',
      responseId,
      model: 'full',
      status: 'error',
      error: `Model ${models.fullModel} not available with current API keys`,
    });
  }

  // Send streaming status
  await sendLLMMessageToMeet({
    type: 'LLM_STATUS',
    responseId,
    model: 'both',
    status: 'streaming',
  });

  // Wait for both to complete (but don't block message handler return)
  Promise.all([fastPromise, fullPromise]).catch((error) => {
    console.error('LLM request error:', error);
  });
}
```
  </action>
  <verify>
Run `npx tsc --noEmit` - no type errors
Run `npm run build` - builds successfully
  </verify>
  <done>
background.ts uses provider abstraction for LLM requests, automatically selecting OpenAI or OpenRouter based on model ID and configured keys
  </done>
</task>

<task type="auto">
  <name>Task 2: Add deprecation notice to OpenRouterClient.ts</name>
  <files>src/services/llm/OpenRouterClient.ts</files>
  <action>
Update `src/services/llm/OpenRouterClient.ts` to add a deprecation notice but keep it functional for backward compatibility:

Add a deprecation JSDoc comment at the top of the file:
```typescript
/**
 * OpenRouter Streaming Client
 *
 * @deprecated Use OpenRouterProvider from './providers' instead.
 * This file is kept for backward compatibility but will be removed in a future version.
 * The provider abstraction supports both OpenRouter and OpenAI with automatic selection.
 *
 * Handles SSE streaming requests to OpenRouter API.
 * Uses eventsource-parser for robust SSE parsing.
 */
```

Add console.warn in streamLLMResponse:
```typescript
export async function streamLLMResponse(options: StreamOptions): Promise<void> {
  console.warn(
    '[DEPRECATED] streamLLMResponse from OpenRouterClient.ts is deprecated. ' +
    'Use the provider abstraction from src/services/llm/providers instead.'
  );
  // ... rest of existing implementation
}
```

This keeps backward compatibility while signaling that direct usage should be migrated.
  </action>
  <verify>
Run `npx tsc --noEmit` - no type errors
Run `npm run build` - builds successfully
  </verify>
  <done>
OpenRouterClient.ts marked as deprecated with notice to use provider abstraction
  </done>
</task>

<task type="checkpoint:human-verify" gate="blocking">
  <what-built>
Multi-provider LLM integration with OpenAI and OpenRouter support:
1. Provider abstraction layer (LLMProvider interface, OpenRouter adapter, OpenAI adapter)
2. Store updates with OpenAI API key support
3. Settings UI with OpenAI key field and provider-grouped model selection
4. Background.ts integration using provider resolution
  </what-built>
  <how-to-verify>
**Setup:**
1. Run `npm run build && npm run dev` to build and watch for changes
2. Load extension in Chrome via "Load unpacked" pointing to dist/ folder

**Test OpenRouter (existing functionality):**
1. Open extension popup -> Settings tab
2. Enter your OpenRouter API key
3. Verify model dropdowns show OpenRouter models grouped under "OpenRouter" optgroup
4. Select a fast model (e.g., Gemini Flash 1.5) and full model (e.g., Claude 3.5 Sonnet)
5. Go to Google Meet (or any meet.google.com/xxx-xxxx-xxx URL)
6. Hold capture hotkey (Ctrl+Shift+Space by default)
7. Release to trigger LLM request
8. Verify fast hint and full answer stream in overlay

**Test OpenAI:**
1. Open extension popup -> Settings tab
2. Enter your OpenAI API key (from platform.openai.com)
3. Verify model dropdowns show OpenAI models under "OpenAI" optgroup
4. Select OpenAI models: fast=GPT-4o Mini, full=GPT-4o
5. Go to Google Meet
6. Hold capture hotkey, release to trigger
7. Verify fast hint and full answer stream (using OpenAI directly, not via OpenRouter)

**Test mixed configuration:**
1. Configure BOTH OpenRouter and OpenAI keys
2. Verify model dropdowns show both provider groups
3. Select OpenRouter fast model and OpenAI full model
4. Trigger LLM request
5. Verify both streams work (fast from OpenRouter, full from OpenAI)

**Test graceful degradation:**
1. Clear OpenAI key, keep only OpenRouter
2. Select an OpenAI-only model (e.g., GPT-4.1)
3. Verify the model appears grayed out with "(requires API key)" indicator
4. Trigger LLM request
5. Verify appropriate error message about unavailable model
  </how-to-verify>
  <resume-signal>Type "approved" if all tests pass, or describe issues found</resume-signal>
</task>

</tasks>

<verification>
1. `npx tsc --noEmit` passes with no errors
2. `npm run build` completes successfully
3. Background.ts imports and uses resolveProviderForModel
4. LLM requests work with OpenRouter API key only
5. LLM requests work with OpenAI API key only
6. LLM requests work with mixed provider configuration
7. Unavailable models show appropriate error messages
</verification>

<success_criteria>
- Provider abstraction integrated into background.ts
- OpenRouterClient.ts marked as deprecated
- LLM requests work with OpenAI key configured
- LLM requests work with OpenRouter key configured
- Mixed provider configurations work (different providers for fast/full)
- Human verification confirms end-to-end functionality
</success_criteria>

<output>
After completion, create `.planning/phases/08-openai-provider-support/08-03-SUMMARY.md`
</output>
