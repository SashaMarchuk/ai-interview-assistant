---
phase: 08-openai-provider-support
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - src/services/llm/types.ts
  - src/services/llm/providers/LLMProvider.ts
  - src/services/llm/providers/OpenRouterProvider.ts
  - src/services/llm/providers/OpenAIProvider.ts
  - src/services/llm/providers/index.ts
  - src/services/llm/index.ts
autonomous: true

must_haves:
  truths:
    - "Provider interface defines streamResponse method with callbacks"
    - "OpenRouter adapter uses existing SSE parsing logic"
    - "OpenAI adapter uses same eventsource-parser for identical SSE format"
    - "Provider registry can resolve provider by ID"
  artifacts:
    - path: "src/services/llm/providers/LLMProvider.ts"
      provides: "LLMProvider interface with ProviderId type"
      exports: ["LLMProvider", "ProviderId", "ProviderStreamOptions", "ModelInfo"]
    - path: "src/services/llm/providers/OpenRouterProvider.ts"
      provides: "OpenRouter adapter implementation"
      exports: ["OpenRouterProvider", "OPENROUTER_MODELS"]
    - path: "src/services/llm/providers/OpenAIProvider.ts"
      provides: "OpenAI adapter implementation"
      exports: ["OpenAIProvider", "OPENAI_MODELS"]
    - path: "src/services/llm/providers/index.ts"
      provides: "Provider registry and factory functions"
      exports: ["getProvider", "resolveActiveProvider", "getAvailableModels"]
  key_links:
    - from: "src/services/llm/providers/OpenRouterProvider.ts"
      to: "eventsource-parser"
      via: "createParser import"
      pattern: "import.*createParser.*eventsource-parser"
    - from: "src/services/llm/providers/OpenAIProvider.ts"
      to: "eventsource-parser"
      via: "createParser import (shared parsing)"
      pattern: "import.*createParser.*eventsource-parser"
    - from: "src/services/llm/providers/index.ts"
      to: "LLMProvider interface"
      via: "Map<ProviderId, LLMProvider>"
      pattern: "providers.*Map"
---

<objective>
Create the LLM provider abstraction layer with interface, OpenRouter adapter, OpenAI adapter, and registry.

Purpose: Enable multi-provider LLM support by defining a common interface and implementing concrete adapters for OpenRouter (existing) and OpenAI (new). This follows the Provider Strategy Pattern recommended in 08-RESEARCH.md.

Output: Provider infrastructure files in src/services/llm/providers/ that can be consumed by background.ts
</objective>

<execution_context>
@/Users/sasha-marchuk/.claude/get-shit-done/workflows/execute-plan.md
@/Users/sasha-marchuk/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/08-openai-provider-support/08-RESEARCH.md

# Existing LLM implementation to refactor from
@src/services/llm/types.ts
@src/services/llm/OpenRouterClient.ts
@src/services/llm/index.ts
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create provider interface and types</name>
  <files>
    src/services/llm/types.ts
    src/services/llm/providers/LLMProvider.ts
  </files>
  <action>
1. Create `src/services/llm/providers/` directory

2. Create `src/services/llm/providers/LLMProvider.ts` with:
   - `ProviderId` type: `'openrouter' | 'openai'`
   - `LLMProvider` interface with:
     - `readonly id: ProviderId`
     - `readonly name: string`
     - `streamResponse(options: ProviderStreamOptions): Promise<void>`
     - `getAvailableModels(): ModelInfo[]`
     - `isModelAvailable(modelId: string): boolean`
   - `ProviderStreamOptions` interface (similar to existing StreamOptions but provider-agnostic):
     - `model: string`
     - `systemPrompt: string`
     - `userPrompt: string`
     - `maxTokens: number`
     - `apiKey: string`
     - `onToken: (token: string) => void`
     - `onComplete: () => void`
     - `onError: (error: Error) => void`
     - `abortSignal?: AbortSignal`
   - `ModelInfo` interface:
     - `id: string` (model identifier)
     - `name: string` (display name)
     - `category: 'fast' | 'full'` (for UI grouping)
     - `provider: ProviderId`

3. Update `src/services/llm/types.ts`:
   - Add `LLMProviderId` type alias to `ProviderId` for backward compatibility
   - Keep existing types (StreamOptions, DualLLMRequest, etc.) - they're still used

Export all types from LLMProvider.ts for external consumption.
  </action>
  <verify>
Run `npx tsc --noEmit` - no type errors in provider types
  </verify>
  <done>
LLMProvider interface exists with all required methods and types are properly exported
  </done>
</task>

<task type="auto">
  <name>Task 2: Create OpenRouter provider adapter</name>
  <files>src/services/llm/providers/OpenRouterProvider.ts</files>
  <action>
Create `src/services/llm/providers/OpenRouterProvider.ts`:

1. Import:
   - `createParser, type EventSourceMessage` from 'eventsource-parser'
   - Types from './LLMProvider'

2. Define `OPENROUTER_API_URL = 'https://openrouter.ai/api/v1/chat/completions'`

3. Define `OPENROUTER_MODELS: ModelInfo[]` with:
   Fast models (category: 'fast'):
   - `{ id: 'google/gemini-flash-1.5', name: 'Gemini Flash 1.5', category: 'fast', provider: 'openrouter' }`
   - `{ id: 'anthropic/claude-3-haiku', name: 'Claude 3 Haiku', category: 'fast', provider: 'openrouter' }`
   - `{ id: 'openai/gpt-4o-mini', name: 'GPT-4o Mini', category: 'fast', provider: 'openrouter' }`

   Full models (category: 'full'):
   - `{ id: 'anthropic/claude-3-5-sonnet', name: 'Claude 3.5 Sonnet', category: 'full', provider: 'openrouter' }`
   - `{ id: 'openai/gpt-4o', name: 'GPT-4o', category: 'full', provider: 'openrouter' }`
   - `{ id: 'google/gemini-pro-1.5', name: 'Gemini Pro 1.5', category: 'full', provider: 'openrouter' }`

4. Create `OpenRouterProvider` class implementing `LLMProvider`:
   - `readonly id = 'openrouter' as const`
   - `readonly name = 'OpenRouter'`
   - `getAvailableModels()`: return OPENROUTER_MODELS
   - `isModelAvailable(modelId)`: check if modelId is in OPENROUTER_MODELS
   - `streamResponse(options)`: Copy logic from existing OpenRouterClient.ts:
     - Build messages array
     - Make fetch request with OpenRouter headers (Authorization, HTTP-Referer, X-Title)
     - Use eventsource-parser for SSE parsing
     - Handle [DONE] marker, parse JSON chunks, extract delta.content

Export `OpenRouterProvider` class and `OPENROUTER_MODELS` array.

This is essentially a class wrapper around the existing OpenRouterClient.ts logic.
  </action>
  <verify>
Run `npx tsc --noEmit` - no type errors in OpenRouterProvider
  </verify>
  <done>
OpenRouterProvider class implements LLMProvider interface and reuses existing SSE parsing logic
  </done>
</task>

<task type="auto">
  <name>Task 3: Create OpenAI provider and registry</name>
  <files>
    src/services/llm/providers/OpenAIProvider.ts
    src/services/llm/providers/index.ts
    src/services/llm/index.ts
  </files>
  <action>
1. Create `src/services/llm/providers/OpenAIProvider.ts`:

Import:
- `createParser, type EventSourceMessage` from 'eventsource-parser'
- Types from './LLMProvider'

Define `OPENAI_API_URL = 'https://api.openai.com/v1/chat/completions'`

Define `OPENAI_MODELS: ModelInfo[]` with:
Fast models (category: 'fast'):
- `{ id: 'gpt-4o-mini', name: 'GPT-4o Mini', category: 'fast', provider: 'openai' }`
- `{ id: 'gpt-4.1-mini', name: 'GPT-4.1 Mini', category: 'fast', provider: 'openai' }`
- `{ id: 'gpt-4.1-nano', name: 'GPT-4.1 Nano', category: 'fast', provider: 'openai' }`

Full models (category: 'full'):
- `{ id: 'gpt-4o', name: 'GPT-4o', category: 'full', provider: 'openai' }`
- `{ id: 'gpt-4.1', name: 'GPT-4.1', category: 'full', provider: 'openai' }`

Create `OpenAIProvider` class implementing `LLMProvider`:
- `readonly id = 'openai' as const`
- `readonly name = 'OpenAI'`
- `getAvailableModels()`: return OPENAI_MODELS
- `isModelAvailable(modelId)`: check if modelId is in OPENAI_MODELS
- `streamResponse(options)`:
  - Build messages array
  - Make fetch request to OPENAI_API_URL with headers:
    - `Authorization: Bearer ${apiKey}`
    - `Content-Type: application/json`
    (Note: No HTTP-Referer or X-Title needed for OpenAI)
  - Use SAME eventsource-parser logic as OpenRouter (SSE format is identical)
  - Handle [DONE] marker, parse JSON chunks, extract delta.content
  - Handle AbortError for cancellation

Export `OpenAIProvider` class and `OPENAI_MODELS` array.

2. Create `src/services/llm/providers/index.ts`:

Import OpenRouterProvider, OpenAIProvider, and types.

Create provider registry:
```typescript
const providers: Map<ProviderId, LLMProvider> = new Map([
  ['openrouter', new OpenRouterProvider()],
  ['openai', new OpenAIProvider()],
]);
```

Export factory functions:
- `getProvider(id: ProviderId): LLMProvider` - Get provider by ID, throw if not found
- `resolveActiveProvider(apiKeys: { openAI?: string; openRouter?: string }): LLMProvider | null` - Auto-detect from keys (priority: OpenAI > OpenRouter)
- `getAvailableModels(apiKeys: { openAI?: string; openRouter?: string }): ModelInfo[]` - Union of models for configured providers
- `resolveProviderForModel(modelId: string, apiKeys: {...}): { provider: LLMProvider; model: string } | null` - Find provider that supports the model

3. Update `src/services/llm/index.ts`:
- Add re-exports from providers/index.ts:
  - `export { getProvider, resolveActiveProvider, getAvailableModels, resolveProviderForModel } from './providers'`
  - `export type { LLMProvider, ProviderId, ProviderStreamOptions, ModelInfo } from './providers/LLMProvider'`
- Keep existing exports (streamLLMResponse, buildPrompt, etc.) for backward compatibility during transition
  </action>
  <verify>
Run `npx tsc --noEmit` - no type errors
Run `npm run build` - builds successfully
  </verify>
  <done>
OpenAI provider created, registry functions exported, barrel export updated with all provider types
  </done>
</task>

</tasks>

<verification>
1. `npx tsc --noEmit` passes with no errors
2. `npm run build` completes successfully
3. Provider interface has all required methods (streamResponse, getAvailableModels, isModelAvailable)
4. Both OpenRouter and OpenAI providers implement the interface
5. Registry exports getProvider, resolveActiveProvider, getAvailableModels functions
6. src/services/llm/index.ts exports provider types and functions
</verification>

<success_criteria>
- LLMProvider interface defined with ProviderId type
- OpenRouterProvider class implements LLMProvider using existing SSE logic
- OpenAIProvider class implements LLMProvider with OpenAI endpoint
- Provider registry with factory functions for provider resolution
- All types properly exported from barrel export
- Build succeeds with no type errors
</success_criteria>

<output>
After completion, create `.planning/phases/08-openai-provider-support/08-01-SUMMARY.md`
</output>
